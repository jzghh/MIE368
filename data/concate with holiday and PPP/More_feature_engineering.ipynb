{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yl7JGhyMyWS0",
        "outputId": "e4693205-3164-4d6f-a4bb-841997fa7124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: events_with_duration_no_end_clean_propagated_holiday_local.csv\n"
          ]
        }
      ],
      "source": [
        "# ====== Normal Holiday concatenation with no dynamic time window ======\n",
        "PATH_PROP = \"events_with_duration_no_end_clean_propagated.csv\"  # your propagated panel\n",
        "PATH_HOL  = \"holidays.csv\"                                      # your holiday file\n",
        "\n",
        "# columns in your holidays.csv (from your file)\n",
        "HOL_COUNTRY_COL = \"Country Code\"\n",
        "HOL_DATE_COL    = \"Date\"        # single holiday date (YYYY-MM-DD / etc.)\n",
        "HOL_NAME_COL    = \"Holiday\"     # optional name; keep None if not needed\n",
        "\n",
        "# holiday window (inclusive). Example: pre=1, post=2 -> [D-1, D+2]\n",
        "PRE_DAYS  = 0\n",
        "POST_DAYS = 4\n",
        "\n",
        "# choose a canonical timezone per country (12 countries you use)\n",
        "COUNTRY_TZ = {\n",
        "    \"AR\": \"America/Argentina/Buenos_Aires\",\n",
        "    \"AU\": \"Australia/Sydney\",\n",
        "    \"BR\": \"America/Sao_Paulo\",\n",
        "    \"CA\": \"America/Toronto\",\n",
        "    \"CN\": \"Asia/Shanghai\",\n",
        "    \"DE\": \"Europe/Berlin\",\n",
        "    \"FR\": \"Europe/Paris\",\n",
        "    \"GB\": \"Europe/London\",\n",
        "    \"JP\": \"Asia/Tokyo\",\n",
        "    \"PL\": \"Europe/Warsaw\",\n",
        "    \"TR\": \"Europe/Istanbul\",\n",
        "    \"US\": \"America/New_York\",\n",
        "}\n",
        "\n",
        "# ====== CODE ======\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from zoneinfo import ZoneInfo\n",
        "from collections import defaultdict\n",
        "\n",
        "def norm_cc(x: str) -> str:\n",
        "    x = str(x).strip().upper()\n",
        "    iso3_to_iso2 = {\"ARG\":\"AR\",\"AUS\":\"AU\",\"BRA\":\"BR\",\"CAN\":\"CA\",\"CHN\":\"CN\",\"DEU\":\"DE\",\n",
        "                    \"FRA\":\"FR\",\"GBR\":\"GB\",\"JPN\":\"JP\",\"POL\":\"PL\",\"TUR\":\"TR\",\"USA\":\"US\"}\n",
        "    name_map = {\n",
        "        \"ARGENTINA\":\"AR\",\"AUSTRALIA\":\"AU\",\"BRAZIL\":\"BR\",\"CANADA\":\"CA\",\"CHINA\":\"CN\",\n",
        "        \"GERMANY\":\"DE\",\"FRANCE\":\"FR\",\"UNITED KINGDOM\":\"GB\",\"UK\":\"GB\",\"JAPAN\":\"JP\",\n",
        "        \"POLAND\":\"PL\",\"TURKEY\":\"TR\",\"TURKIYE\":\"TR\",\"UNITED STATES\":\"US\",\n",
        "        \"UNITED STATES OF AMERICA\":\"US\",\"USA\":\"US\"\n",
        "    }\n",
        "    if len(x) == 2: return x\n",
        "    if len(x) == 3 and x in iso3_to_iso2: return iso3_to_iso2[x]\n",
        "    return name_map.get(x, x)\n",
        "\n",
        "# 1) load events\n",
        "ev = pd.read_csv(PATH_PROP, parse_dates=[\"timestamp\"])\n",
        "ev[\"country\"] = ev[\"country\"].astype(str).str.upper().str.strip()\n",
        "\n",
        "# 2) load holidays and build date windows per country\n",
        "hol = pd.read_csv(PATH_HOL)\n",
        "hol[\"country\"] = hol[HOL_COUNTRY_COL].map(norm_cc)\n",
        "hol[\"holiday_date\"] = pd.to_datetime(hol[HOL_DATE_COL], errors=\"coerce\").dt.date\n",
        "\n",
        "# build per-country calendar (set of all dates covered by [D-PRE, D+POST])\n",
        "cal_dates = {}\n",
        "name_by_date = defaultdict(list)  # optional: collect names per date\n",
        "for cc, g in hol.groupby(\"country\", sort=False):\n",
        "    covered = set()\n",
        "    for _, r in g.iterrows():\n",
        "        if pd.isna(r[\"holiday_date\"]):\n",
        "            continue\n",
        "        start = r[\"holiday_date\"] - pd.Timedelta(days=PRE_DAYS)\n",
        "        end   = r[\"holiday_date\"] + pd.Timedelta(days=POST_DAYS)\n",
        "        dr = pd.date_range(start, end, freq=\"D\").date\n",
        "        covered.update(dr)\n",
        "        if HOL_NAME_COL in hol.columns:\n",
        "            for d in dr:\n",
        "                name_by_date[(cc, d)].append(str(r.get(HOL_NAME_COL, \"\")))\n",
        "    cal_dates[cc] = covered\n",
        "\n",
        "# 3) convert UTC timestamp -> local date per country\n",
        "#    (assume your timestamps are UTC; if not, adjust tz_localize accordingly)\n",
        "parts = []\n",
        "for cc, g in ev.groupby(\"country\", sort=False):\n",
        "    tz = COUNTRY_TZ.get(cc, \"UTC\")\n",
        "    loc_date = g[\"timestamp\"].dt.tz_localize(\"UTC\").dt.tz_convert(ZoneInfo(tz)).dt.date\n",
        "    gg = g.copy()\n",
        "    gg[\"local_date\"] = loc_date\n",
        "    # membership\n",
        "    covered = cal_dates.get(cc, set())\n",
        "    gg[\"is_holiday_local\"] = gg[\"local_date\"].isin(covered)\n",
        "    # optional: holiday names (join multiple names if window expands across >1 holidays)\n",
        "    gg[\"holiday_names_local\"] = [\n",
        "        \", \".join(name_by_date.get((cc, d), [])) if (cc, d) in name_by_date else pd.NA\n",
        "        for d in gg[\"local_date\"]\n",
        "    ]\n",
        "    parts.append(gg)\n",
        "\n",
        "ev_local = pd.concat(parts, ignore_index=True)\n",
        "\n",
        "# 4) save\n",
        "out_path = \"events_with_duration_no_end_clean_propagated_holiday_local.csv\"\n",
        "ev_local.to_csv(out_path, index=False)\n",
        "print(\"Saved:\", out_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Local-time holiday labeling with dynamic windows and NAME CONCATENATION ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from zoneinfo import ZoneInfo\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---- Inputs you already have ----\n",
        "EV_LOCAL_CSV = \"events_with_duration_no_end_clean_propagated_holiday_local (2).csv\"  # or keep ev_local in memory\n",
        "HOLIDAY_CSV  = \"holidays.csv\"\n",
        "OUT_CSV_HOL  = \"events_with_duration_no_end_clean_propagated_holiday_local_v2.csv\"\n",
        "\n",
        "# ---- Global default window used only when no per-holiday rule is found ----\n",
        "DEFAULT_PRE_DAYS  = 0\n",
        "DEFAULT_POST_DAYS = 0\n",
        "\n",
        "# ---- Optional per-holiday windows (fallback rules). Case-insensitive. ----\n",
        "# Key can be a name substring (applies to all countries) OR a (country, exact name) tuple.\n",
        "DYNAMIC_WINDOWS = {\n",
        "    \"CHRISTMAS\": (7, 5),\n",
        "    \"SPRING FESTIVAL\": (1, 5),\n",
        "    \"GOLDEN WEEK\": (1, 5),\n",
        "    \"THANKSGIVING\": (1, 2),\n",
        "    (\"GB\", \"BOXING DAY\"): (1, 2),\n",
        "}\n",
        "\n",
        "# ---- Country -> canonical timezone (adjust if you want different cities) ----\n",
        "COUNTRY_TZ = {\n",
        "    \"AR\": \"America/Argentina/Buenos_Aires\",\n",
        "    \"AU\": \"Australia/Sydney\",\n",
        "    \"BR\": \"America/Sao_Paulo\",\n",
        "    \"CA\": \"America/Toronto\",\n",
        "    \"CN\": \"Asia/Shanghai\",\n",
        "    \"DE\": \"Europe/Berlin\",\n",
        "    \"FR\": \"Europe/Paris\",\n",
        "    \"GB\": \"Europe/London\",\n",
        "    \"JP\": \"Asia/Tokyo\",\n",
        "    \"PL\": \"Europe/Warsaw\",\n",
        "    \"TR\": \"Europe/Istanbul\",\n",
        "    \"US\": \"America/New_York\",\n",
        "}\n",
        "\n",
        "def norm_cc(x: str) -> str:\n",
        "    \"\"\"Normalize country string to ISO2 codes.\"\"\"\n",
        "    x = str(x).strip().upper()\n",
        "    iso3_to_iso2 = {\"ARG\":\"AR\",\"AUS\":\"AU\",\"BRA\":\"BR\",\"CAN\":\"CA\",\"CHN\":\"CN\",\"DEU\":\"DE\",\n",
        "                    \"FRA\":\"FR\",\"GBR\":\"GB\",\"JPN\":\"JP\",\"POL\":\"PL\",\"TUR\":\"TR\",\"USA\":\"US\"}\n",
        "    names = {\n",
        "        \"ARGENTINA\":\"AR\",\"AUSTRALIA\":\"AU\",\"BRAZIL\":\"BR\",\"CANADA\":\"CA\",\"CHINA\":\"CN\",\n",
        "        \"GERMANY\":\"DE\",\"FRANCE\":\"FR\",\"UNITED KINGDOM\":\"GB\",\"UK\":\"GB\",\"JAPAN\":\"JP\",\n",
        "        \"POLAND\":\"PL\",\"TURKEY\":\"TR\",\"TURKIYE\":\"TR\",\"UNITED STATES\":\"US\",\n",
        "        \"UNITED STATES OF AMERICA\":\"US\",\"USA\":\"US\",\n",
        "    }\n",
        "    if len(x) == 2: return x\n",
        "    if len(x) == 3 and x in iso3_to_iso2: return iso3_to_iso2[x]\n",
        "    return names.get(x, x)\n",
        "\n",
        "def pick_window(country: str, name: str, pre_csv, post_csv):\n",
        "    \"\"\"\n",
        "    Decide (pre, post) for one holiday row with the following priority:\n",
        "      1) If the CSV provides 'pre_days'/'post_days', use them.\n",
        "      2) If a per-country rule exists in DYNAMIC_WINDOWS for (country, name), use it.\n",
        "      3) If a name-substring rule exists in DYNAMIC_WINDOWS (case-insensitive), use it.\n",
        "      4) Fallback to DEFAULT_PRE/DEFAULT_POST.\n",
        "    \"\"\"\n",
        "    if pd.notna(pre_csv) or pd.notna(post_csv):\n",
        "        pre  = 0 if pd.isna(pre_csv)  else int(pre_csv)\n",
        "        post = 0 if pd.isna(post_csv) else int(post_csv)\n",
        "        return pre, post\n",
        "\n",
        "    key_country = (country.upper(), str(name).upper())\n",
        "    for k, v in DYNAMIC_WINDOWS.items():\n",
        "        if isinstance(k, tuple):\n",
        "            if k == key_country:\n",
        "                return int(v[0]), int(v[1])\n",
        "\n",
        "    name_up = str(name).upper()\n",
        "    for k, v in DYNAMIC_WINDOWS.items():\n",
        "        if isinstance(k, str) and k in name_up:\n",
        "            return int(v[0]), int(v[1])\n",
        "\n",
        "    return int(DEFAULT_PRE_DAYS), int(DEFAULT_POST_DAYS)\n",
        "\n",
        "# ---- 1) Load events ----\n",
        "try:\n",
        "    ev_local  # use in-memory df if already defined\n",
        "except NameError:\n",
        "    ev_local = pd.read_csv(EV_LOCAL_CSV, parse_dates=[\"timestamp\"])\n",
        "\n",
        "ev_local[\"country\"] = ev_local[\"country\"].astype(str).str.upper().str.strip()\n",
        "\n",
        "# ---- 2) Load holidays ----\n",
        "hol = pd.read_csv(HOLIDAY_CSV)\n",
        "cols = {c.lower(): c for c in hol.columns}\n",
        "cc_col   = cols.get(\"country code\", \"Country Code\") if \"country code\" in cols else \"Country Code\"\n",
        "date_col = cols.get(\"date\", \"Date\")                  if \"date\" in cols        else \"Date\"\n",
        "name_col = cols.get(\"holiday\", \"Holiday\")            if \"holiday\" in cols     else \"Holiday\"\n",
        "\n",
        "# Optional CSV-level per-row windows\n",
        "pre_col  = next((c for c in [\"pre_days\",\"pre\",\"preDays\",\"PreDays\"] if c in hol.columns), None)\n",
        "post_col = next((c for c in [\"post_days\",\"post\",\"postDays\",\"PostDays\"] if c in hol.columns), None)\n",
        "\n",
        "hol[\"country\"] = hol[cc_col].map(norm_cc)\n",
        "hol[\"holiday_name\"] = hol[name_col].astype(str)\n",
        "hol[\"holiday_date\"] = pd.to_datetime(hol[date_col], errors=\"coerce\").dt.date\n",
        "\n",
        "# ---- 3) Build coverage sets per country; collect names PER DATE and CONCAT ----\n",
        "covered_dates = {}                     # country -> set of all covered dates\n",
        "names_by_date = defaultdict(list)      # (country, date) -> list of names (we will de-dup & join)\n",
        "window_by_date = {}                    # (country, date) -> (pre, post) used (max if overlapping)\n",
        "\n",
        "for _, r in hol.dropna(subset=[\"holiday_date\"]).iterrows():\n",
        "    cc   = r[\"country\"]\n",
        "    name = r[\"holiday_name\"]\n",
        "    pre, post = pick_window(cc, name, r[pre_col] if pre_col else pd.NA, r[post_col] if post_col else pd.NA)\n",
        "    start = r[\"holiday_date\"] - pd.Timedelta(days=pre)\n",
        "    end   = r[\"holiday_date\"] + pd.Timedelta(days=post)\n",
        "    for d in pd.date_range(start, end, freq=\"D\").date:\n",
        "        covered_dates.setdefault(cc, set()).add(d)\n",
        "        names_by_date[(cc, d)].append(name)  # collect; we'll concat deduped names later\n",
        "        prev = window_by_date.get((cc, d))\n",
        "        if prev is None:\n",
        "            window_by_date[(cc, d)] = (pre, post)\n",
        "        else:\n",
        "            window_by_date[(cc, d)] = (max(prev[0], pre), max(prev[1], post))\n",
        "\n",
        "# ---- 4) Convert UTC -> local date by country; label membership & CONCAT names ----\n",
        "parts = []\n",
        "for cc, g in ev_local.groupby(\"country\", sort=False):\n",
        "    tz = COUNTRY_TZ.get(cc, \"UTC\")\n",
        "    # If timestamp is tz-naive UTC, localize to UTC then convert; if already tz-aware, just convert\n",
        "    ts = g[\"timestamp\"]\n",
        "    if pd.api.types.is_datetime64tz_dtype(ts):\n",
        "        loc_date = ts.dt.tz_convert(ZoneInfo(tz)).dt.date\n",
        "    else:\n",
        "        loc_date = ts.dt.tz_localize(\"UTC\").dt.tz_convert(ZoneInfo(tz)).dt.date\n",
        "\n",
        "    gg = g.copy()\n",
        "    gg[\"local_date\"] = loc_date\n",
        "    cov = covered_dates.get(cc, set())\n",
        "\n",
        "    gg[\"is_holiday_local\"] = gg[\"local_date\"].isin(cov)\n",
        "\n",
        "    # --- CONCATENATION: for dates covered by one or more holidays, dedupe and join names ---\n",
        "    gg[\"holiday_names_local\"] = [\n",
        "        \", \".join(sorted(set(names_by_date.get((cc, d), [])))) if (cc, d) in names_by_date else pd.NA\n",
        "        for d in gg[\"local_date\"]\n",
        "    ]\n",
        "\n",
        "    # Expose the window actually used for that date (max if overlapping holidays)\n",
        "    gg[\"holiday_window_pre_days\"] = [\n",
        "        window_by_date.get((cc, d), (DEFAULT_PRE_DAYS, DEFAULT_POST_DAYS))[0] for d in gg[\"local_date\"]\n",
        "    ]\n",
        "    gg[\"holiday_window_post_days\"] = [\n",
        "        window_by_date.get((cc, d), (DEFAULT_PRE_DAYS, DEFAULT_POST_DAYS))[1] for d in gg[\"local_date\"]\n",
        "    ]\n",
        "    parts.append(gg)\n",
        "\n",
        "ev_local = pd.concat(parts, ignore_index=True)\n",
        "\n",
        "# ---- 5) Save result with concatenated names ----\n",
        "ev_local.to_csv(OUT_CSV_HOL, index=False)\n",
        "print(\"Saved:\", OUT_CSV_HOL)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0QAafrH8vIL",
        "outputId": "445c2a56-41cc-4546-876b-e86a30dc90d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1852139353.py:132: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
            "  if pd.api.types.is_datetime64tz_dtype(ts):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: events_with_duration_no_end_clean_propagated_holiday_local_v2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== FX builder (auto-detect needed pairs from ev_local & pull from IMF) =====\n",
        "# Comments in ENGLISH.\n",
        "\n",
        "import pandas as pd, numpy as np, re\n",
        "from pathlib import Path\n",
        "\n",
        "EV_LOCAL_PATH = \"events_with_duration_no_end_clean_propagated_holiday_local_v2.csv\"\n",
        "IMF_PATH      = \"dataset_2025-11-07T11_42_37.999821392Z_DEFAULT_INTEGRATION_IMF.STA_ER_4.0.1.csv\"\n",
        "FX_OUT        = \"fx_daily_rates.csv\"\n",
        "\n",
        "# Country -> LCU map (12 countries)\n",
        "LCU = {\"AR\":\"ARS\",\"AU\":\"AUD\",\"BR\":\"BRL\",\"CA\":\"CAD\",\"CN\":\"CNY\",\n",
        "       \"DE\":\"EUR\",\"FR\":\"EUR\",\"GB\":\"GBP\",\"JP\":\"JPY\",\"PL\":\"PLN\",\"TR\":\"TRY\",\"US\":\"USD\"}\n",
        "\n",
        "# For IMF country matching (LCU -> candidate country names)\n",
        "COUNTRY_NAMES_BY_LCU = {\n",
        "    \"ARS\": [\"Argentina\",\"ARGENTINA\"],\n",
        "    \"TRY\": [\"Türkiye\",\"Turkey\",\"TURKEY\",\"Turkey, Republic of\",\"Türkiye, Republic of\"],\n",
        "    \"JPY\": [\"Japan\",\"JAPAN\"],\n",
        "    \"PLN\": [\"Poland\",\"POLAND\"],\n",
        "    \"BRL\": [\"Brazil\",\"BRAZIL\"],\n",
        "    \"CNY\": [\"China\",\"CHINA\"],\n",
        "    \"AUD\": [\"Australia\",\"AUSTRALIA\"],\n",
        "    \"CAD\": [\"Canada\",\"CANADA\"],\n",
        "    \"EUR\": [\"Euro Area\",\"EURO AREA\",\"Germany\",\"GERMANY\",\"France\",\"FRANCE\"],  # safe fallbacks\n",
        "    \"GBP\": [\"United Kingdom\",\"UNITED KINGDOM\",\"UK\",\"GBR\",\"Great Britain\"],\n",
        "    \"USD\": [\"United States\",\"UNITED STATES\",\"USA\",\"United States of America\"]\n",
        "}\n",
        "\n",
        "# 1) detect needed FX pairs from events (currency != LCU)\n",
        "ev = pd.read_csv(EV_LOCAL_PATH, parse_dates=[\"timestamp\"])\n",
        "ev[\"country\"]  = ev[\"country\"].astype(str).str.upper().str.strip()\n",
        "ev[\"currency\"] = ev[\"currency\"].astype(str).str.upper().str.strip()\n",
        "ev[\"LCU\"]      = ev[\"country\"].map(LCU)\n",
        "needs = (ev.loc[ev[\"currency\"]!=ev[\"LCU\"], [\"currency\",\"LCU\"]]\n",
        "           .drop_duplicates()\n",
        "           .rename(columns={\"currency\":\"base\",\"LCU\":\"quote\"}))\n",
        "if needs.empty:\n",
        "    # nothing to build; write an empty-but-valid file\n",
        "    pd.DataFrame(columns=[\"date\",\"base\",\"quote\",\"rate\"]).to_csv(FX_OUT, index=False)\n",
        "    fx_df = pd.read_csv(FX_OUT)  # legacy variable name\n",
        "    print(\"[FX] No FX needed; wrote empty fx_daily_rates.csv\")\n",
        "else:\n",
        "    print(\"[FX] Pairs needed:\", needs.to_dict(orient=\"records\"))\n",
        "\n",
        "    # 2) load IMF wide file\n",
        "    imf = pd.read_csv(IMF_PATH)\n",
        "\n",
        "    def pick_rows(imf_df, country_names):\n",
        "        m = False\n",
        "        for nm in country_names:\n",
        "            m = m | imf_df[\"COUNTRY\"].astype(str).str.fullmatch(nm, case=False, na=False)\n",
        "        return imf_df[m]\n",
        "\n",
        "    def time_columns(df):\n",
        "        pat = re.compile(r\"^(\\d{4})(?:[-]?(Q[1-4]|M\\d{2}))?$|^\\d{4}Q[1-4]$|^\\d{4}M\\d{2}$\")\n",
        "        return [c for c in df.columns if pat.match(str(c))]\n",
        "\n",
        "    def period_to_month_end(s):\n",
        "        s = str(s)\n",
        "        if re.fullmatch(r\"\\d{4}\", s):\n",
        "            y = int(s); return pd.Period(f\"{y}-12\", freq=\"M\").end_time.date()\n",
        "        m = re.match(r\"^(\\d{4})-?Q([1-4])$\", s)\n",
        "        if m:\n",
        "            y, q = int(m.group(1)), int(m.group(2))\n",
        "            month = {1:3,2:6,3:9,4:12}[q]\n",
        "            return pd.Period(f\"{y}-{month:02d}\", freq=\"M\").end_time.date()\n",
        "        m = re.match(r\"^(\\d{4})-?M(\\d{2})$\", s)\n",
        "        if m:\n",
        "            y, mth = int(m.group(1)), int(m.group(2))\n",
        "            return pd.Period(f\"{y}-{mth:02d}\", freq=\"M\").end_time.date()\n",
        "        return None\n",
        "\n",
        "    def imf_series_USD_to_LCU(imf_df, lcu_code):\n",
        "        \"\"\"Return monthly USD->LCU as DataFrame(date, base, quote, rate).\"\"\"\n",
        "        names = COUNTRY_NAMES_BY_LCU.get(lcu_code, [])\n",
        "        sub = pick_rows(imf_df, names)\n",
        "        sub = sub[sub[\"INDICATOR\"].astype(str).str.contains(\"Domestic currency per US Dollar\", case=False, na=False)]\n",
        "        if sub.empty:\n",
        "            return pd.DataFrame(columns=[\"date\",\"base\",\"quote\",\"rate\"])\n",
        "        tcols = time_columns(sub)\n",
        "        if not tcols:\n",
        "            return pd.DataFrame(columns=[\"date\",\"base\",\"quote\",\"rate\"])\n",
        "        long = sub.melt(id_vars=[\"COUNTRY\",\"INDICATOR\"], value_vars=tcols,\n",
        "                        var_name=\"period\", value_name=\"rate\").dropna(subset=[\"rate\"])\n",
        "        long[\"date\"] = long[\"period\"].map(period_to_month_end)\n",
        "        long = (long.dropna(subset=[\"date\"])\n",
        "                    .sort_values(\"date\")\n",
        "                    .drop_duplicates(\"date\", keep=\"last\"))\n",
        "        out = long[[\"date\"]].assign(base=\"USD\", quote=lcu_code,\n",
        "                                    rate=pd.to_numeric(long[\"rate\"], errors=\"coerce\"))\n",
        "        return out.dropna(subset=[\"rate\"]).sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "    def monthly_to_daily(g):\n",
        "        g = g.sort_values(\"date\")\n",
        "        idx = pd.date_range(g[\"date\"].min(), g[\"date\"].max(), freq=\"D\").date\n",
        "        tmp = pd.DataFrame({\"date\": idx})\n",
        "        tmp[\"base\"]  = g[\"base\"].iloc[0]\n",
        "        tmp[\"quote\"] = g[\"quote\"].iloc[0]\n",
        "        tmp = tmp.merge(g[[\"date\",\"rate\"]], on=\"date\", how=\"left\").sort_values(\"date\")\n",
        "        tmp[\"rate\"] = tmp[\"rate\"].ffill()\n",
        "        return tmp\n",
        "\n",
        "    fx_parts = []\n",
        "    for _, row in needs.iterrows():\n",
        "        b, q = row[\"base\"], row[\"quote\"]\n",
        "        if b == \"USD\":\n",
        "            m = imf_series_USD_to_LCU(imf, q)\n",
        "            if m.empty:\n",
        "                print(f\"[WARN] IMF has no USD->{q}. Upload a small CSV with columns [date,base,quote,rate] for USD{q}.\")\n",
        "            else:\n",
        "                fx_parts.append(monthly_to_daily(m))\n",
        "        else:\n",
        "            # generic cross: base->quote = (USD->quote) / (USD->base)\n",
        "            m_q = imf_series_USD_to_LCU(imf, q)\n",
        "            m_b = imf_series_USD_to_LCU(imf, b)\n",
        "            if m_q.empty or m_b.empty:\n",
        "                print(f\"[WARN] Cannot build {b}->{q} via USD cross (missing USD->{q} or USD->{b}).\")\n",
        "            else:\n",
        "                cross = (m_q.merge(m_b, on=\"date\", suffixes=(\"_uq\",\"_ub\"))\n",
        "                           .assign(base=b, quote=q, rate=lambda d: d[\"rate_uq\"]/d[\"rate_ub\"])\n",
        "                           [[\"date\",\"base\",\"quote\",\"rate\"]])\n",
        "                fx_parts.append(monthly_to_daily(cross))\n",
        "\n",
        "    fx_daily = (pd.concat(fx_parts, ignore_index=True) if fx_parts\n",
        "                else pd.DataFrame(columns=[\"date\",\"base\",\"quote\",\"rate\"]))\n",
        "    fx_daily.to_csv(FX_OUT, index=False)\n",
        "    fx_df = pd.read_csv(FX_OUT)  # legacy variable for old code paths\n",
        "    print(\"Saved FX file:\", FX_OUT, \"| rows:\", len(fx_daily),\n",
        "          \"| pairs:\", fx_daily[[\"base\",\"quote\"]].drop_duplicates().shape[0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFtHdhUzEsr9",
        "outputId": "055d6f55-81bc-4ef3-b369-34903bc771f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FX] Pairs needed: [{'base': 'USD', 'quote': 'ARS'}, {'base': 'USD', 'quote': 'TRY'}]\n",
            "Saved FX file: fx_daily_rates.csv | rows: 54514 | pairs: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Stretch fx_daily_rates.csv to cover full event date range ---\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "EV = \"events_with_duration_no_end_clean_propagated_holiday_local_v2.csv\"\n",
        "FX = \"fx_daily_rates.csv\"\n",
        "\n",
        "ev = pd.read_csv(EV, parse_dates=[\"timestamp\"])\n",
        "dmin, dmax = ev[\"timestamp\"].dt.date.min(), ev[\"timestamp\"].dt.date.max()\n",
        "\n",
        "fx = pd.read_csv(FX)\n",
        "fx.columns = [c.lower() for c in fx.columns]\n",
        "date_col = \"date\" if \"date\" in fx.columns else (\"day\" if \"day\" in fx.columns else \"timestamp\")\n",
        "fx[date_col] = pd.to_datetime(fx[date_col], errors=\"coerce\").dt.date\n",
        "fx[\"base\"]   = fx[\"base\"].astype(str).str.upper().str.strip()\n",
        "fx[\"quote\"]  = fx[\"quote\"].astype(str).str.upper().str.strip()\n",
        "fx[\"rate\"]   = pd.to_numeric(fx[\"rate\"], errors=\"coerce\")\n",
        "fx = fx.dropna(subset=[date_col,\"base\",\"quote\",\"rate\"]).drop_duplicates([date_col,\"base\",\"quote\"], keep=\"last\")\n",
        "\n",
        "def stretch(g):\n",
        "    idx = pd.date_range(dmin, dmax, freq=\"D\").date\n",
        "    out = pd.DataFrame({\"date\": idx})\n",
        "    out[\"base\"]  = g[\"base\"].iloc[0]\n",
        "    out[\"quote\"] = g[\"quote\"].iloc[0]\n",
        "    out = out.merge(g[[date_col,\"rate\"]].rename(columns={date_col:\"date\"}), on=\"date\", how=\"left\")\n",
        "    out[\"rate\"] = out[\"rate\"].ffill().bfill()  # as-of: use last known, bfill for very early events\n",
        "    return out\n",
        "\n",
        "fx_long = (fx.sort_values(date_col)\n",
        "             .groupby([\"base\",\"quote\"], group_keys=False)\n",
        "             .apply(stretch))\n",
        "\n",
        "fx_long.to_csv(FX, index=False)\n",
        "print(\"FX stretched to:\", dmax, \"| rows:\", len(fx_long))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxzpCPBrRzPj",
        "outputId": "1dd18f3d-c261-4a2d-daa7-61256599a500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FX stretched to: 2025-11-05 | rows: 7268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1482923390.py:30: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(stretch))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FX coverage audit ---\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "EV = \"events_with_duration_no_end_clean_propagated_holiday_local_v2.csv\"\n",
        "FX = \"fx_daily_rates.csv\"\n",
        "\n",
        "LCU = {\"AR\":\"ARS\",\"AU\":\"AUD\",\"BR\":\"BRL\",\"CA\":\"CAD\",\"CN\":\"CNY\",\n",
        "       \"DE\":\"EUR\",\"FR\":\"EUR\",\"GB\":\"GBP\",\"JP\":\"JPY\",\"PL\":\"PLN\",\"TR\":\"TRY\",\"US\":\"USD\"}\n",
        "\n",
        "ev = pd.read_csv(EV, parse_dates=[\"timestamp\"])\n",
        "ev[\"country\"]  = ev[\"country\"].astype(str).str.upper().str.strip()\n",
        "ev[\"currency\"] = ev[\"currency\"].astype(str).str.upper().str.strip()\n",
        "ev[\"LCU\"]      = ev[\"country\"].map(LCU)\n",
        "ev[\"event_date\"] = ev[\"timestamp\"].dt.date\n",
        "\n",
        "need = (ev.loc[ev[\"currency\"]!=ev[\"LCU\"], [\"currency\",\"LCU\"]]\n",
        "          .drop_duplicates().rename(columns={\"currency\":\"base\",\"LCU\":\"quote\"}))\n",
        "print(\"Pairs needed:\", need.to_dict(orient=\"records\"))\n",
        "\n",
        "fx = pd.read_csv(FX)\n",
        "fx.columns = [c.lower() for c in fx.columns]\n",
        "date_col = \"date\" if \"date\" in fx.columns else (\"day\" if \"day\" in fx.columns else \"timestamp\")\n",
        "fx[date_col] = pd.to_datetime(fx[date_col], errors=\"coerce\").dt.date\n",
        "fx[\"base\"]   = fx[\"base\"].astype(str).str.upper().str.strip()\n",
        "fx[\"quote\"]  = fx[\"quote\"].astype(str).str.upper().str.strip()\n",
        "fx[\"rate\"]   = pd.to_numeric(fx[\"rate\"], errors=\"coerce\")\n",
        "fx = fx.dropna(subset=[date_col,\"rate\"]).drop_duplicates([date_col,\"base\",\"quote\"], keep=\"last\")\n",
        "\n",
        "# per-pair date span\n",
        "span = (fx.groupby([\"base\",\"quote\"])[date_col]\n",
        "          .agg([\"min\",\"max\",\"count\"]).reset_index().sort_values([\"base\",\"quote\"]))\n",
        "print(span)\n",
        "\n",
        "# uncovered rows by exact match\n",
        "sub = ev[ev[\"currency\"]!=ev[\"LCU\"]][[\"event_date\",\"currency\",\"LCU\"]].copy()\n",
        "k = pd.MultiIndex.from_frame(sub.rename(columns={\"currency\":\"base\",\"LCU\":\"quote\"}))\n",
        "fx_map = fx.set_index([date_col,\"base\",\"quote\"])[\"rate\"]\n",
        "miss = sub.index[fx_map.reindex(k).isna()]\n",
        "print(\"Exact-match missing rows:\", len(miss))\n",
        "\n",
        "# show where they are\n",
        "if len(miss):\n",
        "    audit = (ev.loc[miss, [\"country\",\"currency\",\"LCU\",\"event_date\"]]\n",
        "               .value_counts().reset_index(name=\"n\").sort_values(\"n\", ascending=False))\n",
        "    print(audit.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP5uc6_qSFQ3",
        "outputId": "4ff10a3f-d03f-43d2-9a27-4a193582845d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pairs needed: [{'base': 'USD', 'quote': 'ARS'}, {'base': 'USD', 'quote': 'TRY'}]\n",
            "  base quote         min         max  count\n",
            "0  USD   ARS  2015-11-25  2025-11-05   3634\n",
            "1  USD   TRY  2015-11-25  2025-11-05   3634\n",
            "Exact-match missing rows: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== PPP block (FX -> LCU -> PPP -> US parity) =====================\n",
        "# Robust, suffix-free (no _x/_y), with AS-OF fallback for FX coverage.\n",
        "# Comments in ENGLISH.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# -------- CONFIG --------\n",
        "PATH_EV_LOCAL_FALLBACK = \"events_with_duration_no_end_clean_propagated_holiday_local_v2.csv\"\n",
        "PATH_PPP = \"PPP__LCU_per_int___-_12_Countries_-_Latest.csv\"\n",
        "PATH_FX  = \"fx_daily_rates.csv\"\n",
        "\n",
        "OUT_CSV         = \"events_with_duration_no_end_clean_propagated_holiday_local_ppp_parity.csv\"\n",
        "NEEDS_FX_REPORT = \"rows_needing_fx.csv\"\n",
        "\n",
        "# Country -> Local Currency (LCU) for the 12 markets in your project\n",
        "CCY_LCU = {\n",
        "    \"AR\":\"ARS\",\"AU\":\"AUD\",\"BR\":\"BRL\",\"CA\":\"CAD\",\"CN\":\"CNY\",\n",
        "    \"DE\":\"EUR\",\"FR\":\"EUR\",\"GB\":\"GBP\",\"JP\":\"JPY\",\"PL\":\"PLN\",\"TR\":\"TRY\",\"US\":\"USD\",\n",
        "}\n",
        "\n",
        "# -------- Helpers --------\n",
        "def norm_cc(x: str) -> str:\n",
        "    \"\"\"Normalize country name/code to ISO2 used in your project.\"\"\"\n",
        "    x = str(x).strip().upper()\n",
        "    iso3_to_iso2 = {\"ARG\":\"AR\",\"AUS\":\"AU\",\"BRA\":\"BR\",\"CAN\":\"CA\",\"CHN\":\"CN\",\"DEU\":\"DE\",\n",
        "                    \"FRA\":\"FR\",\"GBR\":\"GB\",\"JPN\":\"JP\",\"POL\":\"PL\",\"TUR\":\"TR\",\"USA\":\"US\"}\n",
        "    names = {\n",
        "        \"ARGENTINA\":\"AR\",\"AUSTRALIA\":\"AU\",\"BRAZIL\":\"BR\",\"CANADA\":\"CA\",\"CHINA\":\"CN\",\n",
        "        \"GERMANY\":\"DE\",\"FRANCE\":\"FR\",\"UNITED KINGDOM\":\"GB\",\"UK\":\"GB\",\"JAPAN\":\"JP\",\n",
        "        \"POLAND\":\"PL\",\"TURKEY\":\"TR\",\"TURKIYE\":\"TR\",\"TÜRKIYE\":\"TR\",\n",
        "        \"UNITED STATES\":\"US\",\"UNITED STATES OF AMERICA\":\"US\",\"USA\":\"US\",\n",
        "        \"EURO AREA\":\"DE\"  # safe fallback to EUR anchor when PPP file uses 'Euro Area'\n",
        "    }\n",
        "    if len(x) == 2:\n",
        "        return x\n",
        "    if len(x) == 3 and x in iso3_to_iso2:\n",
        "        return iso3_to_iso2[x]\n",
        "    return names.get(x, x)\n",
        "\n",
        "def load_ev_local() -> pd.DataFrame:\n",
        "    \"\"\"Use in-memory ev_local if present; else load from fallback CSV.\"\"\"\n",
        "    if \"ev_local\" in globals():\n",
        "        df = ev_local.copy()\n",
        "    else:\n",
        "        df = pd.read_csv(PATH_EV_LOCAL_FALLBACK, parse_dates=[\"timestamp\"])\n",
        "    # normalize minimal schema\n",
        "    df[\"appid\"]   = df[\"appid\"].astype(str).str.strip()\n",
        "    df[\"country\"] = df[\"country\"].astype(str).str.upper().str.strip()\n",
        "    if \"currency\" not in df.columns:\n",
        "        raise ValueError(\"`ev_local` must contain a 'currency' column for FX->LCU.\")\n",
        "    df[\"currency\"] = df[\"currency\"].astype(str).str.upper().str.strip()\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df[\"timestamp\"]):\n",
        "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def load_ppp(path_ppp: str) -> pd.DataFrame:\n",
        "    \"\"\"Load PPP and keep the latest per country. Returns ['country','ppp_lcu_per_int_dollar'].\"\"\"\n",
        "    ppp_raw = pd.read_csv(path_ppp)\n",
        "\n",
        "    # detect the numeric PPP column robustly\n",
        "    ppp_val_col = None\n",
        "    for c in ppp_raw.columns:\n",
        "        cl = str(c).lower()\n",
        "        if (\"lcu\" in cl and \"int\" in cl) or (\"ppp\" in cl and \"lcu\" in cl):\n",
        "            ppp_val_col = c; break\n",
        "    if ppp_val_col is None:\n",
        "        num_cols = [c for c in ppp_raw.columns if pd.api.types.is_numeric_dtype(ppp_raw[c])]\n",
        "        if not num_cols:\n",
        "            raise ValueError(\"Cannot detect PPP numeric column.\")\n",
        "        ppp_val_col = num_cols[0]\n",
        "\n",
        "    # detect country column\n",
        "    cc_col = next((n for n in [\"Country Code\",\"country_code\",\"ISO3\",\"iso3\",\"Code\",\"code\",\n",
        "                               \"Country\",\"Country Name\",\"name\"] if n in ppp_raw.columns), None)\n",
        "    if cc_col is None:\n",
        "        raise ValueError(\"Cannot detect PPP country column.\")\n",
        "\n",
        "    # optional year/period to pick latest\n",
        "    year_col = next((n for n in [\"Year\",\"year\",\"Time\",\"time\",\"Date\",\"date\",\"Period\",\"period\"]\n",
        "                     if n in ppp_raw.columns), None)\n",
        "\n",
        "    p = ppp_raw.copy()\n",
        "    p[\"country\"] = p[cc_col].map(norm_cc)\n",
        "    p[\"ppp_lcu_per_int_dollar\"] = pd.to_numeric(p[ppp_val_col], errors=\"coerce\")\n",
        "    if year_col is not None:\n",
        "        p[\"_ppp_year\"] = pd.to_numeric(p[year_col], errors=\"coerce\")\n",
        "        p = (p.sort_values([\"country\",\"_ppp_year\"], ascending=[True,False])\n",
        "               .drop_duplicates(subset=[\"country\"], keep=\"first\"))\n",
        "    p = p.dropna(subset=[\"country\",\"ppp_lcu_per_int_dollar\"])\n",
        "    return p[[\"country\",\"ppp_lcu_per_int_dollar\"]]\n",
        "\n",
        "def load_fx_map(path_fx: str):\n",
        "    \"\"\"\n",
        "    Load FX file if present and non-empty.\n",
        "    Expected columns (case-insensitive): date, base, quote, rate\n",
        "    Return a MultiIndex Series: (date, base, quote) -> rate; or None if unavailable.\n",
        "    \"\"\"\n",
        "    p = Path(path_fx)\n",
        "    if (not p.exists()) or (p.stat().st_size == 0):\n",
        "        return None\n",
        "    fx = pd.read_csv(p)\n",
        "    if fx.empty:\n",
        "        return None\n",
        "    cols = {c.lower(): c for c in fx.columns}\n",
        "    date_col  = cols.get(\"date\")  or cols.get(\"day\") or cols.get(\"timestamp\")\n",
        "    base_col  = cols.get(\"base\")\n",
        "    quote_col = cols.get(\"quote\")\n",
        "    rate_col  = cols.get(\"rate\")  or cols.get(\"px\") or cols.get(\"value\")\n",
        "    if not all([date_col, base_col, quote_col, rate_col]):\n",
        "        raise ValueError(\"FX file must have columns: date, base, quote, rate.\")\n",
        "    fx[date_col]  = pd.to_datetime(fx[date_col], errors=\"coerce\")\n",
        "    fx[base_col]  = fx[base_col].astype(str).str.upper().str.strip()\n",
        "    fx[quote_col] = fx[quote_col].astype(str).str.upper().str.strip()\n",
        "    fx[rate_col]  = pd.to_numeric(fx[rate_col], errors=\"coerce\")\n",
        "    fx = fx.dropna(subset=[date_col, rate_col])\n",
        "    fx[\"date_only\"] = fx[date_col].dt.date\n",
        "    return fx.set_index([\"date_only\", base_col, quote_col])[rate_col]\n",
        "\n",
        "def fx_to_lcu_vectorized(sub: pd.DataFrame, fx_map: pd.Series) -> np.ndarray:\n",
        "    \"\"\"Vectorized exact-match lookup; if missing, try inverse and reciprocal.\"\"\"\n",
        "    k_direct  = pd.MultiIndex.from_frame(sub[[\"event_date\",\"currency\",\"LCU\"]])\n",
        "    k_inverse = pd.MultiIndex.from_frame(sub[[\"event_date\",\"LCU\",\"currency\"]])\n",
        "    r_direct  = fx_map.reindex(k_direct).to_numpy()\n",
        "    r_inverse = fx_map.reindex(k_inverse).to_numpy()\n",
        "    return np.where(~pd.isna(r_direct), r_direct,\n",
        "                    np.where(~pd.isna(r_inverse), 1.0 / r_inverse, np.nan))\n",
        "\n",
        "def asof_fill_fx(ev: pd.DataFrame, fx_path: str) -> None:\n",
        "    \"\"\"\n",
        "    AS-OF fallback: for remaining rows with missing FX, fill by nearest\n",
        "    historical (backward) rate; if still missing (event earlier than first FX),\n",
        "    try forward.\n",
        "    This function mutates 'ev' inplace on column 'fx_to_lcu'.\n",
        "    \"\"\"\n",
        "    fx = pd.read_csv(fx_path)\n",
        "    if fx.empty:\n",
        "        return\n",
        "    fx.columns = [c.lower() for c in fx.columns]\n",
        "    date_col  = \"date\" if \"date\" in fx.columns else (\"day\" if \"day\" in fx.columns else \"timestamp\")\n",
        "    base_col, quote_col = \"base\", \"quote\"\n",
        "    rate_col = \"rate\" if \"rate\" in fx.columns else (\"px\" if \"px\" in fx.columns else \"value\")\n",
        "\n",
        "    fx[date_col]  = pd.to_datetime(fx[date_col], errors=\"coerce\")\n",
        "    fx[base_col]  = fx[base_col].astype(str).str.upper().str.strip()\n",
        "    fx[quote_col] = fx[quote_col].astype(str).str.upper().str.strip()\n",
        "    fx[rate_col]  = pd.to_numeric(fx[rate_col], errors=\"coerce\")\n",
        "    fx = fx.dropna(subset=[date_col, rate_col]).sort_values(date_col)\n",
        "\n",
        "    need_mask = (~ev[\"currency\"].eq(ev[\"LCU\"])) & (ev[\"fx_to_lcu\"].isna())\n",
        "    if not need_mask.any():\n",
        "        return\n",
        "\n",
        "    need_idx = ev.index[need_mask]\n",
        "    pairs = ev.loc[need_idx, [\"currency\",\"LCU\"]].drop_duplicates().itertuples(index=False)\n",
        "\n",
        "    for base_ccy, quote_ccy in pairs:\n",
        "        idx_pair = need_idx[(ev.loc[need_idx,\"currency\"]==base_ccy) &\n",
        "                            (ev.loc[need_idx,\"LCU\"]==quote_ccy)]\n",
        "        fx_pair = fx[(fx[base_col]==base_ccy) & (fx[quote_col]==quote_ccy)][[date_col, rate_col]].copy()\n",
        "        if fx_pair.empty:\n",
        "            fx_inv = fx[(fx[base_col]==quote_ccy) & (fx[quote_col]==base_ccy)][[date_col, rate_col]].copy()\n",
        "            if not fx_inv.empty:\n",
        "                fx_inv = fx_inv.rename(columns={rate_col:\"_r\"})\n",
        "                fx_pair = fx_inv.copy()\n",
        "                fx_pair[rate_col] = 1.0 / fx_inv[\"_r\"]\n",
        "                fx_pair.drop(columns=[\"_r\"], inplace=True)\n",
        "        if fx_pair.empty:\n",
        "            continue\n",
        "\n",
        "        sub = ev.loc[idx_pair, [\"event_date\"]].copy()\n",
        "        sub[\"event_date\"] = pd.to_datetime(sub[\"event_date\"], errors=\"coerce\")\n",
        "        fx_pair = fx_pair.sort_values(date_col)\n",
        "\n",
        "        m  = pd.merge_asof(sub.sort_values(\"event_date\"), fx_pair, left_on=\"event_date\",\n",
        "                           right_on=date_col, direction=\"backward\")\n",
        "        r  = m[rate_col].to_numpy()\n",
        "\n",
        "        if np.isnan(r).any():\n",
        "            m2 = pd.merge_asof(sub.sort_values(\"event_date\"), fx_pair, left_on=\"event_date\",\n",
        "                               right_on=date_col, direction=\"forward\")\n",
        "            r  = np.where(np.isnan(r), m2[rate_col].to_numpy(), r)\n",
        "\n",
        "        ev.loc[idx_pair, \"fx_to_lcu\"] = r\n",
        "\n",
        "# -------- MAIN --------\n",
        "# A) Load base table + PPP (suffix-free)\n",
        "ev_local = load_ev_local()\n",
        "ppp_use  = load_ppp(PATH_PPP)  # ['country','ppp_lcu_per_int_dollar']\n",
        "\n",
        "# Clean any lingering PPP columns from previous runs\n",
        "ppp_cols_to_drop = [c for c in ev_local.columns if c.startswith(\"ppp_lcu_per_int_dollar\")]\n",
        "if ppp_cols_to_drop:\n",
        "    ev_local = ev_local.drop(columns=ppp_cols_to_drop)\n",
        "\n",
        "# Attach PPP by mapping (one-to-one by country)\n",
        "ppp_map = (ppp_use\n",
        "           .drop_duplicates(subset=[\"country\"], keep=\"first\")\n",
        "           .set_index(\"country\")[\"ppp_lcu_per_int_dollar\"]\n",
        "           .to_dict())\n",
        "ev_local[\"ppp_lcu_per_int_dollar\"] = ev_local[\"country\"].map(ppp_map)\n",
        "\n",
        "print(f\"[PPP] attached via map; missing (no PPP): {int(ev_local['ppp_lcu_per_int_dollar'].isna().sum())}\")\n",
        "\n",
        "# B) LCU & date & price/regular hygiene\n",
        "ev_local[\"LCU\"] = ev_local[\"country\"].map(CCY_LCU)\n",
        "ev_local[\"event_date\"] = ev_local[\"timestamp\"].dt.date\n",
        "if \"regular\" not in ev_local.columns:\n",
        "    ev_local[\"regular\"] = np.nan\n",
        "\n",
        "# C) FX exact-match first\n",
        "if \"fx_to_lcu\" in ev_local.columns:\n",
        "    ev_local[\"fx_to_lcu\"] = pd.to_numeric(ev_local[\"fx_to_lcu\"], errors=\"coerce\")\n",
        "else:\n",
        "    ev_local[\"fx_to_lcu\"] = np.nan\n",
        "\n",
        "same_ccy = ev_local[\"currency\"].eq(ev_local[\"LCU\"])\n",
        "ev_local.loc[same_ccy, \"fx_to_lcu\"] = 1.0\n",
        "\n",
        "fx_map = load_fx_map(PATH_FX)\n",
        "need_fx_mask = (~same_ccy) & ev_local[\"fx_to_lcu\"].isna()\n",
        "if fx_map is not None and need_fx_mask.any():\n",
        "    sub = ev_local.loc[need_fx_mask, [\"event_date\",\"currency\",\"LCU\"]]\n",
        "    ev_local.loc[need_fx_mask, \"fx_to_lcu\"] = fx_to_lcu_vectorized(sub, fx_map)\n",
        "\n",
        "# D) AS-OF fallback for remaining FX gaps (backward->forward)\n",
        "asof_fill_fx(ev_local, PATH_FX)\n",
        "\n",
        "# E) Compute LCU amounts for PPP\n",
        "if \"price_lcu_for_ppp\" not in ev_local.columns:\n",
        "    ev_local[\"price_lcu_for_ppp\"] = np.nan\n",
        "if \"regular_lcu_for_ppp\" not in ev_local.columns:\n",
        "    ev_local[\"regular_lcu_for_ppp\"] = np.nan\n",
        "\n",
        "can_mul_p = ev_local[\"price_lcu_for_ppp\"].isna() & ev_local[\"fx_to_lcu\"].notna()\n",
        "ev_local.loc[can_mul_p, \"price_lcu_for_ppp\"] = ev_local.loc[can_mul_p, \"price\"] * ev_local.loc[can_mul_p, \"fx_to_lcu\"]\n",
        "\n",
        "can_mul_r = ev_local[\"regular_lcu_for_ppp\"].isna() & ev_local[\"fx_to_lcu\"].notna() & ev_local[\"regular\"].notna()\n",
        "ev_local.loc[can_mul_r, \"regular_lcu_for_ppp\"] = ev_local.loc[can_mul_r, \"regular\"] * ev_local.loc[can_mul_r, \"fx_to_lcu\"]\n",
        "\n",
        "# F) PPP normalization (LCU -> international $)\n",
        "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "    ev_local[\"price_ppp_intd\"]   = ev_local[\"price_lcu_for_ppp\"]   / ev_local[\"ppp_lcu_per_int_dollar\"]\n",
        "    ev_local[\"regular_ppp_intd\"] = ev_local[\"regular_lcu_for_ppp\"] / ev_local[\"ppp_lcu_per_int_dollar\"]\n",
        "    ev_local[\"ppp_price_over_regular_intd\"] = ev_local[\"price_ppp_intd\"] / ev_local[\"regular_ppp_intd\"]\n",
        "\n",
        "# G) US parity (no merge, suffix-free)\n",
        "#    1) drop lingering parity columns from previous runs\n",
        "ev_local = ev_local.drop(columns=[c for c in ev_local.columns\n",
        "                                  if c.startswith(\"price_ppp_intd_US\")\n",
        "                                  or c == \"ppp_parity_vs_us\"], errors=\"ignore\")\n",
        "#    2) US series indexed by (appid, timestamp)\n",
        "us_series = (ev_local.loc[ev_local[\"country\"]==\"US\", [\"appid\",\"timestamp\",\"price_ppp_intd\"]]\n",
        "             .dropna(subset=[\"price_ppp_intd\"])\n",
        "             .drop_duplicates(subset=[\"appid\",\"timestamp\"], keep=\"last\")\n",
        "             .set_index([\"appid\",\"timestamp\"])[\"price_ppp_intd\"])\n",
        "#    3) reindex to all rows\n",
        "keys = pd.MultiIndex.from_arrays([ev_local[\"appid\"].astype(str), ev_local[\"timestamp\"]])\n",
        "ev_local[\"price_ppp_intd_US\"] = us_series.reindex(keys).to_numpy()\n",
        "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "    ev_local[\"ppp_parity_vs_us\"] = ev_local[\"price_ppp_intd\"] / ev_local[\"price_ppp_intd_US\"]\n",
        "\n",
        "# H) Diagnostics & optional export of missing-FX rows\n",
        "needs_fx = (~same_ccy) & ev_local[\"fx_to_lcu\"].isna()\n",
        "print(f\"[QC] Missing PPP rows (no PPP): {int(ev_local['ppp_lcu_per_int_dollar'].isna().sum())}\")\n",
        "print(f\"[QC] Rows still needing FX (currency!=LCU but fx_to_lcu is NaN): {int(needs_fx.sum())}\")\n",
        "\n",
        "if needs_fx.any():\n",
        "    cols = [\"appid\",\"country\",\"timestamp\",\"currency\",\"LCU\",\"price\",\"regular\"]\n",
        "    ev_local.loc[needs_fx, cols].to_csv(NEEDS_FX_REPORT, index=False)\n",
        "    print(f\"[WARN] Exported rows needing FX to: {NEEDS_FX_REPORT}\")\n",
        "\n",
        "# I) Optional sanity checks\n",
        "dups = ev_local.duplicated([\"appid\",\"country\",\"timestamp\"]).sum()\n",
        "print(\"[QC] Duplicates (appid,country,timestamp):\", int(dups))\n",
        "us_rows = ev_local[\"country\"].eq(\"US\") & ev_local[\"price_ppp_intd_US\"].notna()\n",
        "if us_rows.any():\n",
        "    diff = np.nanmax(np.abs(ev_local.loc[us_rows, \"ppp_parity_vs_us\"] - 1.0))\n",
        "    print(f\"[QC] Max |US parity - 1|: {diff:.3e}\")\n",
        "\n",
        "# J) Save final CSV\n",
        "ev_local.to_csv(OUT_CSV, index=False)\n",
        "print(\"Saved:\", OUT_CSV)\n",
        "print(\"Added columns:\",\n",
        "      [c for c in [\"LCU\",\"event_date\",\"fx_to_lcu\",\"price_lcu_for_ppp\",\"regular_lcu_for_ppp\",\n",
        "                   \"ppp_lcu_per_int_dollar\",\"price_ppp_intd\",\"regular_ppp_intd\",\n",
        "                   \"ppp_price_over_regular_intd\",\"price_ppp_intd_US\",\"ppp_parity_vs_us\"]\n",
        "       if c in ev_local.columns])\n",
        "# ===================== END PPP block =====================\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyDx_49w5Bv7",
        "outputId": "33114333-539d-47eb-dee3-9dcd890448f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PPP] attached via map; missing (no PPP): 0\n",
            "[QC] Missing PPP rows (no PPP): 0\n",
            "[QC] Rows still needing FX (currency!=LCU but fx_to_lcu is NaN): 0\n",
            "[QC] Duplicates (appid,country,timestamp): 0\n",
            "[QC] Max |US parity - 1|: 0.000e+00\n",
            "Saved: events_with_duration_no_end_clean_propagated_holiday_local_ppp_parity.csv\n",
            "Added columns: ['LCU', 'event_date', 'fx_to_lcu', 'price_lcu_for_ppp', 'regular_lcu_for_ppp', 'ppp_lcu_per_int_dollar', 'price_ppp_intd', 'regular_ppp_intd', 'ppp_price_over_regular_intd', 'price_ppp_intd_US', 'ppp_parity_vs_us']\n"
          ]
        }
      ]
    }
  ]
}