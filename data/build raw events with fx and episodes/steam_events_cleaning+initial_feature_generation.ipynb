{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR5b6N3a3Zbv"
      },
      "outputs": [],
      "source": [
        "!pip -q install requests tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"ITAD_API_KEY\"] = \"ae3e5f344491238a7acbb2c5850f62f8d9d1aa47\""
      ],
      "metadata": {
        "id": "Ctoc3I2v7_Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Module 0 — Setup & Config (API key + JSON config)\n",
        "# =========================================================\n",
        "# If running in Colab, uncomment:\n",
        "# !pip -q install requests tqdm\n",
        "\n",
        "import os, time, json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- ITAD API constants ---\n",
        "ITAD_BASE = \"https://api.isthereanydeal.com\"\n",
        "STEAM_SHOP_ID = 61  # Steam\n",
        "\n",
        "# --- Config JSON: write once if missing, else read ---\n",
        "CFG_PATH = Path(\"steam_panel_config.json\")\n",
        "DEFAULT_CFG = {\n",
        "    \"countries\": [\"AR\",\"AU\",\"BR\",\"CA\",\"CN\",\"DE\",\"FR\",\"GB\",\"JP\",\"PL\",\"TR\",\"US\"],\n",
        "    \"weeks_back\": 104,\n",
        "    \"label_horizon_weeks\": 8\n",
        "}\n",
        "if not CFG_PATH.exists():\n",
        "    CFG_PATH.write_text(json.dumps(DEFAULT_CFG, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "cfg = json.loads(CFG_PATH.read_text())\n",
        "COUNTRIES = cfg[\"countries\"]\n",
        "WEEKS_BACK = int(cfg[\"weeks_back\"])\n",
        "\n",
        "# --- ITAD API key (prefer environment variable) ---\n",
        "# Example for Colab: os.environ[\"ITAD_API_KEY\"] = \"YOUR_KEY\"\n",
        "os.environ.setdefault(\"ITAD_API_KEY\", \"ae3e5f344491238a7acbb2c5850f62f8d9d1aa47\")\n",
        "ITAD_API_KEY = os.getenv(\"ITAD_API_KEY\")\n",
        "if not ITAD_API_KEY or ITAD_API_KEY == \"REPLACE_ME_WITH_YOUR_ITAD_KEY\":\n",
        "    raise ValueError(\"Set ITAD_API_KEY via environment or inline string before running.\")\n",
        "\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.width = 160\n"
      ],
      "metadata": {
        "id": "lXQTKE6M8uvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Module 1 — Read CSVs and build the release dimension\n",
        "# Inputs required: steam_top_sellers.csv, steam_prices.csv\n",
        "# =========================================================\n",
        "\n",
        "def read_csv_flexible(path_main: str, alt_names: List[str] = None) -> pd.DataFrame:\n",
        "    \"\"\"Try main path first; if not found, try alternates.\"\"\"\n",
        "    alt_names = alt_names or []\n",
        "    try:\n",
        "        return pd.read_csv(path_main)\n",
        "    except FileNotFoundError:\n",
        "        for alt in alt_names:\n",
        "            try:\n",
        "                return pd.read_csv(alt)\n",
        "            except FileNotFoundError:\n",
        "                pass\n",
        "        raise\n",
        "\n",
        "# Must contain at least `appid`\n",
        "df_sellers = read_csv_flexible(\"steam_top_sellers.csv\", [\"steam_top_sellers (1).csv\"])\n",
        "# Should contain release info per appid + (country/cc)\n",
        "df_prices  = read_csv_flexible(\"steam_prices_normalized.csv\", [\"steam_prices_normalized (1).csv\"])\n",
        "\n",
        "df_sellers.columns = [c.strip() for c in df_sellers.columns]\n",
        "df_prices.columns  = [c.strip() for c in df_prices.columns]\n",
        "\n",
        "def build_release_dim(df_prices: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Normalize release info into a stable schema:\n",
        "    [appid, country, release_time, release_price, release_currency, release_country].\n",
        "\n",
        "    Column mapping (priority order):\n",
        "      - release_time   <= release_date_iso (ISO) or release_date (legacy human format)\n",
        "      - release_price  <= initial\n",
        "      - release_currency <= currency\n",
        "      - country        <= country (or 'cc' if present)\n",
        "    \"\"\"\n",
        "    rel = df_prices.copy()\n",
        "    rel.columns = [c.strip() for c in rel.columns]\n",
        "\n",
        "    # --- Map to canonical names (use ISO first, then legacy) ---\n",
        "    rename_map = {}\n",
        "    if \"release_date_iso\" in rel.columns:\n",
        "        rename_map[\"release_date_iso\"] = \"release_time\"\n",
        "    elif \"release_date\" in rel.columns:\n",
        "        rename_map[\"release_date\"] = \"release_time\"\n",
        "\n",
        "    if \"initial\" in rel.columns:\n",
        "        rename_map[\"initial\"] = \"release_price\"\n",
        "    if \"currency\" in rel.columns:\n",
        "        rename_map[\"currency\"] = \"release_currency\"\n",
        "    if \"cc\" in rel.columns and \"country\" not in rel.columns:\n",
        "        rename_map[\"cc\"] = \"country\"\n",
        "\n",
        "    rel = rel.rename(columns=rename_map)\n",
        "\n",
        "    # --- Types & basic cleaning ---\n",
        "    rel[\"appid\"] = (\n",
        "        pd.to_numeric(rel[\"appid\"], errors=\"coerce\")\n",
        "          .astype(\"Int64\")\n",
        "          .astype(str)\n",
        "    )\n",
        "\n",
        "    if \"country\" in rel.columns:\n",
        "        rel[\"country\"] = rel[\"country\"].astype(str).str.upper()\n",
        "    else:\n",
        "        rel[\"country\"] = pd.NA\n",
        "\n",
        "    if \"release_currency\" in rel.columns:\n",
        "        rel[\"release_currency\"] = rel[\"release_currency\"].astype(str).str.upper()\n",
        "\n",
        "    # Default release_country to country if not explicitly provided\n",
        "    rel[\"release_country\"] = rel.get(\"release_country\", rel.get(\"country\"))\n",
        "\n",
        "    # --- Parse release_time robustly ---\n",
        "    if \"release_time\" in rel.columns:\n",
        "        # ISO parses naturally; legacy strings will be coerced if any remain.\n",
        "        rel[\"release_time\"] = (\n",
        "            pd.to_datetime(rel[\"release_time\"], errors=\"coerce\", utc=True)\n",
        "              .dt.tz_convert(None)\n",
        "        )\n",
        "\n",
        "    if \"release_price\" in rel.columns:\n",
        "        rel[\"release_price\"] = pd.to_numeric(rel[\"release_price\"], errors=\"coerce\")\n",
        "\n",
        "    # --- Keep canonical columns only ---\n",
        "    cols = [\"appid\", \"country\", \"release_time\",\n",
        "            \"release_price\", \"release_currency\", \"release_country\"]\n",
        "    rel = rel[[c for c in cols if c in rel.columns]]\n",
        "\n",
        "    # --- One row per (appid, country); choose earliest release_time when available ---\n",
        "    if \"release_time\" in rel.columns:\n",
        "        rel = (rel.sort_values([\"appid\", \"country\", \"release_time\"])\n",
        "                 .drop_duplicates([\"appid\", \"country\"], keep=\"first\"))\n",
        "    else:\n",
        "        rel = rel.drop_duplicates([\"appid\", \"country\"], keep=\"first\")\n",
        "\n",
        "    return rel\n",
        "\n"
      ],
      "metadata": {
        "id": "m6c9nddB89wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Build and persist release_dim (restart-safe artifact) ---\n",
        "rel = build_release_dim(df_prices)\n",
        "\n",
        "# Basic hygiene\n",
        "need = {\"appid\",\"country\",\"release_time\",\"release_price\",\"release_currency\"}\n",
        "assert need.issubset(set(rel.columns)), f\"release_dim missing: {need - set(rel.columns)}\"\n",
        "dup = rel.duplicated([\"appid\",\"country\"]).sum()\n",
        "assert dup == 0, f\"release_dim has {dup} duplicate (appid,country) keys\"\n",
        "\n",
        "# Persist both parquet (preferred) and csv (fallback)\n",
        "rel.to_parquet(\"release_dim.parquet\", index=False)\n",
        "rel.to_csv(\"release_dim.csv\", index=False)\n",
        "print(\"[M1] release_dim saved -> release_dim.parquet (and csv); shape:\", rel.shape)\n",
        "\n",
        "\n",
        "\n",
        "# --- Build or recover `appids` for lookup (robust to kernel restarts) ---\n",
        "# Try using an existing `appids`; if missing, rebuild from sellers CSV.\n",
        "try:\n",
        "    appids  # if already defined by Module 1, this will succeed\n",
        "except NameError:\n",
        "    # Re-load sellers if needed (works even if M1 wasn't run)\n",
        "    try:\n",
        "        df_sellers\n",
        "    except NameError:\n",
        "        df_sellers = pd.read_csv(\"steam_top_sellers.csv\")\n",
        "        df_sellers.columns = [c.strip() for c in df_sellers.columns]\n",
        "\n",
        "    # Normalize to string appids safely (handles floats like 12345.0)\n",
        "    appids = (\n",
        "        pd.to_numeric(df_sellers[\"appid\"], errors=\"coerce\")\n",
        "          .dropna()\n",
        "          .astype(int)\n",
        "          .astype(str)\n",
        "          .unique()\n",
        "          .tolist()\n",
        "    )\n",
        "\n",
        "print(f\"[M2] total appids for lookup: {len(appids)}\")\n",
        "assert len(appids) > 0, \"No appids found for /lookup. Check steam_top_sellers.csv.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKHHtadloNcG",
        "outputId": "3d9cc85b-e2a0-4538-b5e8-fac1d04eeb50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[M1] release_dim saved -> release_dim.parquet (and csv); shape: (513, 6)\n",
            "[M2] total appids for lookup: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Module 2 — Map appid -> gid (Steam, shop_id=61)\n",
        "# =========================================================\n",
        "\n",
        "\n",
        "\n",
        "def lookup_gids_for_appids(appids: List[str], shop_id: int = STEAM_SHOP_ID,\n",
        "                           batch: int = 100) -> Dict[str, str]:\n",
        "    \"\"\"POST /lookup/id/shop/{shop_id}/v1 to translate 'app/{appid}' to GIDs.\"\"\"\n",
        "    url = f\"{ITAD_BASE}/lookup/id/shop/{shop_id}/v1\"\n",
        "    out = {}\n",
        "    for i in tqdm(range(0, len(appids), batch), desc=\"Lookup GIDs\"):\n",
        "        chunk = appids[i:i+batch]\n",
        "        payload = [f\"app/{a}\" for a in chunk]\n",
        "        r = requests.post(url, params={\"key\": ITAD_API_KEY}, json=payload, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        for k, v in data.items():\n",
        "            app = k.split(\"/\", 1)[1]  # \"app/12345\" -> \"12345\"\n",
        "            if v:\n",
        "                out[app] = v\n",
        "        time.sleep(0.2)  # gentle throttle\n",
        "    return out\n",
        "\n",
        "gid_map = lookup_gids_for_appids(appids)\n",
        "df_gid  = pd.DataFrame({\"appid\": list(gid_map.keys()), \"gid\": list(gid_map.values())})\n",
        "print(\"Mapped:\", len(df_gid), \"/\", len(appids))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fylrDF979oBt",
        "outputId": "1f8c0e75-cd27-45b8-de58-0b6724640e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Lookup GIDs: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapped: 50 / 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Module 3 — Pull ITAD history and flatten to raw events (Steam only)\n",
        "# =========================================================\n",
        "\n",
        "def _extract_events_list(data) -> list:\n",
        "    \"\"\"Tolerant extractor for event arrays nested under several possible keys.\"\"\"\n",
        "    if isinstance(data, dict):\n",
        "        for k in (\"data\",\"history\",\"list\",\"events\"):\n",
        "            if k in data and isinstance(data[k], list):\n",
        "                return data[k]\n",
        "        return []\n",
        "    return data or []\n",
        "\n",
        "def _amount(d: Optional[dict]) -> Optional[float]:\n",
        "    \"\"\"Normalize ITAD amount fields to float or None.\"\"\"\n",
        "    if not isinstance(d, dict):\n",
        "        return None\n",
        "    if d.get(\"amount\") is not None:\n",
        "        return float(d[\"amount\"])\n",
        "    if d.get(\"amountInt\") is not None:\n",
        "        return float(d[\"amountInt\"]) / 100.0\n",
        "    return None\n",
        "\n",
        "def fetch_history(gid: str, country: str, since_days: int = 365*12,\n",
        "                  max_retries=3, backoff=0.8) -> list:\n",
        "    url = f\"{ITAD_BASE}/games/history/v2\"\n",
        "    since = (\n",
        "        datetime.now(timezone.utc) - timedelta(days=since_days)\n",
        "    ).isoformat(timespec=\"seconds\").replace(\"+00:00\",\"Z\")\n",
        "\n",
        "    def do_req(params: dict) -> list:\n",
        "        attempt = 0\n",
        "        while True:\n",
        "            try:\n",
        "                r = requests.get(url, params=params, timeout=30)\n",
        "                r.raise_for_status()\n",
        "                return _extract_events_list(r.json())\n",
        "            except requests.HTTPError as e:\n",
        "                code = e.response.status_code if e.response is not None else None\n",
        "                if code and 500 <= code < 600 and attempt < max_retries:\n",
        "                    time.sleep(backoff * (2**attempt)); attempt += 1; continue\n",
        "                raise\n",
        "\n",
        "    # Try with country\n",
        "    params = {\"key\": ITAD_API_KEY, \"id\": gid, \"country\": country.lower(), \"since\": since}\n",
        "    events = do_req(params)\n",
        "    # Fallback without country if empty\n",
        "    if not events:\n",
        "        params2 = {\"key\": ITAD_API_KEY, \"id\": gid, \"since\": since}\n",
        "        events = do_req(params2)\n",
        "    return events\n",
        "\n",
        "def flatten_events(gid: str, country: str, events: list, appid: str) -> list:\n",
        "    \"\"\"Turn ITAD events into a tidy row list.\"\"\"\n",
        "    rows = []\n",
        "    for e in events or []:\n",
        "        ts = pd.to_datetime(\n",
        "            e.get(\"timestamp\") or e.get(\"recorded\") or e.get(\"time\"),\n",
        "            utc=True, errors=\"coerce\"\n",
        "        )\n",
        "        if pd.isna(ts):\n",
        "            continue\n",
        "        deal = e.get(\"deal\") or {}\n",
        "        shop = e.get(\"shop\") or {}\n",
        "        price   = _amount(deal.get(\"price\"))\n",
        "        regular = _amount(deal.get(\"regular\"))\n",
        "        cut_raw = deal.get(\"cut\")\n",
        "        cut     = float(cut_raw) if cut_raw is not None else None\n",
        "        curr    = (deal.get(\"price\") or {}).get(\"currency\")\n",
        "        shop_id = int(shop.get(\"id\") or 0)\n",
        "        rows.append({\n",
        "            \"appid\": str(appid),\n",
        "            \"gid\": str(gid),\n",
        "            \"country\": country,\n",
        "            \"timestamp\": ts.tz_convert(None),\n",
        "            \"price\": None if price   is None else float(price),\n",
        "            \"regular\": None if regular is None else float(regular),\n",
        "            \"cut\": cut,\n",
        "            \"currency\": curr,\n",
        "            \"shop_id\": shop_id\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "def collect_events_for_universe(df_gid: pd.DataFrame, countries: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"Fetch and flatten events for all (appid x country).\"\"\"\n",
        "    event_rows = []\n",
        "    for _, r in tqdm(df_gid.iterrows(), total=len(df_gid), desc=\"Fetch events\"):\n",
        "        gid, appid = r[\"gid\"], str(r[\"appid\"])\n",
        "        for cc in countries:\n",
        "            ev = fetch_history(gid, cc, since_days=365*12)\n",
        "            event_rows += flatten_events(gid, cc, ev, appid)\n",
        "    df_events = pd.DataFrame(event_rows)\n",
        "    if df_events.empty:\n",
        "        return df_events\n",
        "    df_events = df_events[df_events[\"shop_id\"] == STEAM_SHOP_ID].copy()\n",
        "    df_events[\"country\"] = df_events[\"country\"].str.upper()\n",
        "    df_events.sort_values([\"appid\",\"country\",\"timestamp\"], inplace=True)\n",
        "    return df_events\n",
        "\n",
        "df_events = collect_events_for_universe(df_gid, COUNTRIES)\n",
        "print(\"Raw events pulled (Steam only):\", df_events.shape)\n",
        "df_events.to_csv(\"steam_events_raw.csv\", index=False)  # audit/reuse\n"
      ],
      "metadata": {
        "id": "Rl1W8_U799wy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d1901c7-385d-4194-acc9-1b00790ed393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetch events: 100%|██████████| 50/50 [08:42<00:00, 10.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw events pulled (Steam only): (20416, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Module 4 — Join release info & create light event labels\n",
        "#   - SALE / BASE_PRICE_CHANGE / CURRENCY_CHANGE / OTHER\n",
        "#   - No thresholding here and no static baseline price.\n",
        "#   - Robust to pre-release drop and currency boundaries.\n",
        "# =========================================================\n",
        "# --- Module-4 bootstrap: ensure release_dim is available ---\n",
        "import os, pandas as pd\n",
        "import numpy as np\n",
        "def ensure_release_dim(df_prices: pd.DataFrame = None) -> pd.DataFrame:\n",
        "    \"\"\"Load release_dim from artifact, or rebuild from df_prices if needed.\"\"\"\n",
        "    # 1) use in-memory var if present\n",
        "    if \"release_dim\" in globals():\n",
        "        return globals()[\"release_dim\"]\n",
        "\n",
        "    # 2) try persisted artifact\n",
        "    if os.path.exists(\"release_dim.parquet\"):\n",
        "        rel = pd.read_parquet(\"release_dim.parquet\")\n",
        "        print(\"[M4] loaded release_dim.parquet:\", rel.shape)\n",
        "        return rel\n",
        "    if os.path.exists(\"release_dim.csv\"):\n",
        "        rel = pd.read_csv(\"release_dim.csv\", parse_dates=[\"release_time\"])\n",
        "        print(\"[M4] loaded release_dim.csv:\", rel.shape)\n",
        "        return rel\n",
        "\n",
        "    # 3) rebuild from prices if provided\n",
        "    if df_prices is not None:\n",
        "        rel = build_release_dim(df_prices)\n",
        "        print(\"[M4] rebuilt release_dim from df_prices:\", rel.shape)\n",
        "        return rel\n",
        "\n",
        "    raise FileNotFoundError(\n",
        "        \"release_dim not in memory and no artifact on disk. \"\n",
        "        \"Run Module 1 or pass df_prices to rebuild.\"\n",
        "    )\n",
        "\n",
        "# If you already have df_prices in memory, you can pass it; else leave None.\n",
        "release_dim = ensure_release_dim(df_prices=None)\n",
        "\n",
        "# Final sanity + sort for stable merges\n",
        "release_dim[\"appid\"] = release_dim[\"appid\"].astype(str)\n",
        "release_dim[\"country\"] = release_dim[\"country\"].astype(str).str.upper().str.strip()\n",
        "release_dim = release_dim.sort_values([\"appid\",\"country\"])\n",
        "\n",
        "print(\"[M4] release_dim ready. shape=\", release_dim.shape)\n",
        "\n",
        "# --- now continue Module 4 as usual ---\n",
        "df_raw_enriched = attach_release_info(df_events, release_dim)\n",
        "\n",
        "\n",
        "# -----------------------------------------------\n",
        "# 0) Join release info\n",
        "# -----------------------------------------------\n",
        "def attach_release_info(df_events: pd.DataFrame, release_dim: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Left-join release info, then guarantee presence of release_* columns with safe defaults:\n",
        "      - release_time      -> NaT if missing\n",
        "      - release_price     -> NaN if missing\n",
        "      - release_currency  -> fallback to current event currency\n",
        "      - release_country   -> fallback to current event country\n",
        "    \"\"\"\n",
        "    out = df_events.copy()\n",
        "    out[\"appid\"] = out[\"appid\"].astype(str)\n",
        "    out[\"country\"] = out[\"country\"].astype(str).str.upper()\n",
        "\n",
        "    out = out.merge(release_dim, on=[\"appid\", \"country\"], how=\"left\", validate=\"m:1\")\n",
        "\n",
        "    # Safety nets: ensure downstream never KeyErrors on these columns\n",
        "    if \"release_time\" not in out.columns:\n",
        "        out[\"release_time\"] = pd.NaT\n",
        "    if \"release_price\" not in out.columns:\n",
        "        out[\"release_price\"] = np.nan\n",
        "    if \"release_currency\" not in out.columns:\n",
        "        out[\"release_currency\"] = out.get(\"currency\")  # same currency => deltas are meaningful\n",
        "    if \"release_country\" not in out.columns:\n",
        "        out[\"release_country\"] = out.get(\"country\")\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# -----------------------------------------------\n",
        "# 1) Primary labeling (pre-drop, light rules)\n",
        "# -----------------------------------------------\n",
        "def label_events_raw(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Boolean-only labeling.\n",
        "\n",
        "    Steps:\n",
        "      0) Clean price/regular (≤0 treated as missing) and drop rows where both are NaN\n",
        "      0b) Normalize currency fields to string to avoid tri-state booleans\n",
        "      1) Flag sale\n",
        "      2) Flag currency change (including group-start currency mismatch vs release currency)\n",
        "      3) Base price change within same-currency stretches (initial pass; will be recomputed post-drop)\n",
        "      4) Release-day flags\n",
        "      5) Release-relative deltas (computed only when currencies match)\n",
        "    \"\"\"\n",
        "    gcols = [\"appid\", \"country\"]\n",
        "    df = df_raw.copy()\n",
        "\n",
        "    # --- 0) Clean + drop pure-noise rows ---\n",
        "    for c in (\"price\", \"regular\"):\n",
        "        df[c] = pd.to_numeric(df.get(c), errors=\"coerce\")\n",
        "        df.loc[df[c] <= 0, c] = pd.NA\n",
        "    df = df[~(df[\"price\"].isna() & df[\"regular\"].isna())].copy()\n",
        "\n",
        "    # --- 0b) Normalize currency fields to string (robust comparisons) ---\n",
        "    for c in (\"currency\", \"release_currency\"):\n",
        "        if c in df.columns:\n",
        "            df[c] = df[c].astype(str)\n",
        "\n",
        "    # Stable ordering for groupwise logic\n",
        "    df = df.sort_values(gcols + [\"timestamp\"], kind=\"mergesort\").copy()\n",
        "\n",
        "    # --- 1) Sale rule ---\n",
        "    df[\"is_sale\"] = (\n",
        "        pd.to_numeric(df.get(\"cut\"), errors=\"coerce\").fillna(0) > 0\n",
        "    ) | (\n",
        "        df[\"price\"].notna() & df[\"regular\"].notna() & (df[\"price\"] < df[\"regular\"])\n",
        "    )\n",
        "\n",
        "    # --- 2) Currency change flags ---\n",
        "    df[\"prev_currency\"] = df.groupby(gcols, sort=False)[\"currency\"].shift()\n",
        "    df[\"is_currency_change\"] = (\n",
        "        df[\"currency\"].notna() & df[\"prev_currency\"].notna() &\n",
        "        df[\"currency\"].ne(df[\"prev_currency\"])\n",
        "    )\n",
        "\n",
        "    # Group-start mismatch vs release currency -> mark as at_start\n",
        "    df[\"_is_group_start\"] = df.groupby(gcols, sort=False).cumcount().eq(0)\n",
        "    df[\"is_currency_change_at_start\"] = (\n",
        "        df[\"_is_group_start\"] &\n",
        "        df[\"currency\"].notna() &\n",
        "        df.get(\"release_currency\").notna() &\n",
        "        df[\"currency\"].ne(df[\"release_currency\"])\n",
        "    )\n",
        "    # Consider group-start mismatch as currency change too (for completeness)\n",
        "    df.loc[df[\"is_currency_change_at_start\"], \"is_currency_change\"] = True\n",
        "\n",
        "    # --- 3) Initial base price change (will be recomputed post-drop anyway) ---\n",
        "    df[\"is_base_change\"] = False\n",
        "    for (_, _), g in df.groupby(gcols, sort=False):\n",
        "        anchor = None\n",
        "        last_ccy = None\n",
        "        for idx, r in g.iterrows():\n",
        "            ccy = r[\"currency\"]\n",
        "            reg = r[\"regular\"]\n",
        "\n",
        "            # Reset on currency boundary or detected currency change\n",
        "            if (last_ccy is None) or (ccy != last_ccy) \\\n",
        "               or bool(r[\"is_currency_change\"]) or bool(r[\"is_currency_change_at_start\"]):\n",
        "                anchor = None\n",
        "                last_ccy = ccy\n",
        "\n",
        "            # Only consider non-sale rows with a valid regular price\n",
        "            if (not bool(r[\"is_sale\"])) and pd.notna(reg):\n",
        "                if anchor is None:\n",
        "                    anchor = float(reg)  # first anchor is not a change\n",
        "                else:\n",
        "                    thr = max(0.1, 0.01 * anchor)  # ignore tiny ticks\n",
        "                    if abs(float(reg) - anchor) > thr:\n",
        "                        df.at[idx, \"is_base_change\"] = True\n",
        "                        anchor = float(reg)\n",
        "                    else:\n",
        "                        anchor = float(reg)\n",
        "\n",
        "    # --- 4) Release-day flags ---\n",
        "    ts_day  = pd.to_datetime(df[\"timestamp\"]).dt.normalize()\n",
        "    rel_day = pd.to_datetime(df.get(\"release_time\")).dt.normalize()\n",
        "    df[\"is_release_day\"] = rel_day.notna() & ts_day.eq(rel_day)\n",
        "    df[\"is_release_day_sale\"] = df[\"is_release_day\"] & df[\"is_sale\"]\n",
        "    df[\"is_release_day_base_change\"] = df[\"is_release_day\"] & df[\"is_base_change\"]\n",
        "\n",
        "    # --- 5) Deltas vs release (only when currencies match) ---\n",
        "    df[\"days_since_release\"] = (ts_day - rel_day).dt.days\n",
        "    same_ccy = df[\"currency\"].eq(df.get(\"release_currency\")).fillna(False)\n",
        "    df[\"delta_from_release_price\"] = np.where(\n",
        "        same_ccy, df[\"price\"] - df[\"release_price\"], np.nan\n",
        "    )\n",
        "    df[\"delta_pct_from_release\"] = np.where(\n",
        "        same_ccy & df[\"release_price\"].gt(0).fillna(False),\n",
        "        df[\"price\"] / df[\"release_price\"] - 1.0,\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    # Cleanup temp\n",
        "    df.drop(columns=[\"prev_currency\", \"_is_group_start\"], inplace=True, errors=\"ignore\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# -----------------------------------------------\n",
        "# 2) Post-drop helpers\n",
        "# -----------------------------------------------\n",
        "def reflag_group_start_currency_mismatch(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    After any row filtering (e.g., dropping pre-release), group starts change.\n",
        "    Recompute 'is_currency_change_at_start' at the new group heads and\n",
        "    ensure 'is_currency_change' is also True for those rows.\n",
        "    \"\"\"\n",
        "    d = df.copy()\n",
        "    for c in (\"currency\", \"release_currency\"):\n",
        "        if c in d.columns:\n",
        "            d[c] = d[c].astype(str)\n",
        "\n",
        "    gcols = [\"appid\",\"country\"]\n",
        "    d = d.sort_values(gcols + [\"timestamp\"], kind=\"mergesort\").copy()\n",
        "    d[\"_is_group_start_new\"] = d.groupby(gcols, sort=False).cumcount().eq(0)\n",
        "\n",
        "    at_start = (\n",
        "        d[\"_is_group_start_new\"] &\n",
        "        d[\"currency\"].notna() &\n",
        "        d.get(\"release_currency\").notna() &\n",
        "        d[\"currency\"].ne(d[\"release_currency\"])\n",
        "    )\n",
        "\n",
        "    # Set/overwrite the flags at new group heads\n",
        "    if \"is_currency_change_at_start\" not in d.columns:\n",
        "        d[\"is_currency_change_at_start\"] = False\n",
        "    d.loc[:, \"is_currency_change_at_start\"] = d[\"is_currency_change_at_start\"] | at_start\n",
        "\n",
        "    if \"is_currency_change\" not in d.columns:\n",
        "        d[\"is_currency_change\"] = False\n",
        "    d.loc[at_start, \"is_currency_change\"] = True\n",
        "\n",
        "    d.drop(columns=[\"_is_group_start_new\"], inplace=True, errors=\"ignore\")\n",
        "    return d\n",
        "\n",
        "\n",
        "def recompute_is_base_change(df, abs_thr=0.10, rel_thr=0.01) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Recompute `is_base_change` within (appid, country) using an anchor-based logic:\n",
        "      - Only evaluate on NON-sale rows with valid `regular`.\n",
        "      - Reset anchor on currency boundary or explicit currency-change flags.\n",
        "      - A change is flagged if |regular - anchor| > max(abs_thr, rel_thr * anchor).\n",
        "    \"\"\"\n",
        "    d = df.copy()\n",
        "\n",
        "    # Ensure numeric + sanitize non-positive regulars\n",
        "    if \"regular\" in d.columns:\n",
        "        d[\"regular\"] = pd.to_numeric(d[\"regular\"], errors=\"coerce\")\n",
        "        d.loc[d[\"regular\"] <= 0, \"regular\"] = np.nan\n",
        "\n",
        "    # Normalize currency fields to string\n",
        "    for c in (\"currency\", \"release_currency\"):\n",
        "        if c in d.columns:\n",
        "            d[c] = d[c].astype(str)\n",
        "\n",
        "    gcols = [\"appid\", \"country\"]\n",
        "    if not set(gcols).issubset(d.columns):\n",
        "        raise ValueError(\"Missing required grouping columns: appid/country\")\n",
        "    d = d.sort_values(gcols + [\"timestamp\"], kind=\"mergesort\").copy()\n",
        "\n",
        "    def _flags(group: pd.DataFrame) -> pd.Series:\n",
        "        anchor = None\n",
        "        last_ccy = None\n",
        "        out = []\n",
        "        for _, r in group.iterrows():\n",
        "            ccy = str(r.get(\"currency\"))\n",
        "            reg = r.get(\"regular\")\n",
        "            is_sale = bool(r.get(\"is_sale\", False))\n",
        "            ccy_changed = bool(r.get(\"is_currency_change\", False)) or bool(r.get(\"is_currency_change_at_start\", False))\n",
        "\n",
        "            # Reset anchor on currency boundary or currency-change flags\n",
        "            if (last_ccy is None) or (ccy != last_ccy) or ccy_changed:\n",
        "                anchor = None\n",
        "                last_ccy = ccy\n",
        "\n",
        "            flag = False\n",
        "            if (not is_sale) and pd.notna(reg):\n",
        "                reg = float(reg)\n",
        "                thr = max(abs_thr, rel_thr * anchor) if anchor is not None else None\n",
        "                if anchor is None:\n",
        "                    anchor = reg\n",
        "                else:\n",
        "                    if abs(reg - anchor) > thr:\n",
        "                        flag = True\n",
        "                        anchor = reg\n",
        "                    else:\n",
        "                        anchor = reg\n",
        "            out.append(flag)\n",
        "        return pd.Series(out, index=group.index)\n",
        "\n",
        "    d[\"is_base_change\"] = (\n",
        "        d.groupby(gcols, sort=False, group_keys=False)\n",
        "         .apply(_flags)\n",
        "         .astype(bool)\n",
        "    )\n",
        "    return d\n",
        "\n",
        "\n",
        "def neutralize_release_day_mismatch(df):\n",
        "    \"\"\"\n",
        "    If the currency on release-day differs from the release currency,\n",
        "    mark `is_currency_change_at_start=True` and null out delta columns to keep same-currency convention.\n",
        "    \"\"\"\n",
        "    d = df.copy()\n",
        "    needed = {\"is_release_day\", \"currency\", \"release_currency\"}\n",
        "    if not needed.issubset(d.columns):\n",
        "        return d\n",
        "\n",
        "    d[\"currency\"] = d[\"currency\"].astype(str)\n",
        "    d[\"release_currency\"] = d[\"release_currency\"].astype(str)\n",
        "\n",
        "    mask = d[\"is_release_day\"].fillna(False) & (d[\"currency\"] != d[\"release_currency\"])\n",
        "    if \"is_currency_change_at_start\" in d.columns:\n",
        "        d.loc[mask, \"is_currency_change_at_start\"] = True\n",
        "    else:\n",
        "        d[\"is_currency_change_at_start\"] = False\n",
        "        d.loc[mask, \"is_currency_change_at_start\"] = True\n",
        "\n",
        "    for c in (\"delta_from_release_price\", \"delta_pct_from_release\"):\n",
        "        if c in d.columns:\n",
        "            d.loc[mask, c] = np.nan\n",
        "    return d\n",
        "\n",
        "\n",
        "# -----------------------------------------------\n",
        "# From M3 to M4: attach + label, then persist\n",
        "# (Only the post-drop pipeline has a small insertion)\n",
        "# -----------------------------------------------\n",
        "\n",
        "# 1) Attach release info\n",
        "df_raw_enriched = attach_release_info(df_events, release_dim)\n",
        "\n",
        "# 2) Primary labels (pre-drop)\n",
        "df_raw_enriched = label_events_raw(df_raw_enriched)\n",
        "\n",
        "# 3) Remove F2P rows (unchanged)\n",
        "F2P_APPIDS = {\"1085660\",\"1222670\",\"230410\",\"236390\",\"2767030\",\"730\"}\n",
        "df_raw_enriched = df_raw_enriched[~df_raw_enriched[\"appid\"].isin(F2P_APPIDS)].copy()\n",
        "\n",
        "# 4) Drop pre-release rows (unchanged)\n",
        "df_raw_enriched = df_raw_enriched[\n",
        "    df_raw_enriched[\"release_time\"].isna() |\n",
        "    (df_raw_enriched[\"timestamp\"] >= df_raw_enriched[\"release_time\"])\n",
        "].copy()\n",
        "\n",
        "# 5) NEW: Re-flag group-start currency mismatch AFTER the drop\n",
        "#    (fixes the 36/51 start-of-group currency mismatch that were not flagged)\n",
        "df_raw_enriched = reflag_group_start_currency_mismatch(df_raw_enriched)\n",
        "\n",
        "# 6) Recompute base changes (anchor method) on the post-drop table (unchanged intent)\n",
        "df_raw_enriched = recompute_is_base_change(df_raw_enriched, abs_thr=0.10, rel_thr=0.01)\n",
        "\n",
        "# 7) (Optional) Neutralize deltas for release-day currency mismatch (unchanged)\n",
        "df_raw_enriched = neutralize_release_day_mismatch(df_raw_enriched)\n",
        "\n",
        "# 8) Persist (unchanged)\n",
        "df_raw_enriched.to_csv(\"steam_events_raw_enriched.csv\", index=False)\n",
        "df_raw_enriched = pd.read_csv(\"steam_events_raw_enriched.csv\", parse_dates=[\"timestamp\",\"release_time\"])\n",
        "\n",
        "# 9) Guardrails (unchanged)\n",
        "need = {\n",
        "    \"is_sale\",\"is_currency_change\",\"is_currency_change_at_start\",\n",
        "    \"is_base_change\",\"is_release_day\",\"is_release_day_sale\",\n",
        "    \"is_release_day_base_change\",\"days_since_release\",\n",
        "    \"delta_from_release_price\",\"delta_pct_from_release\",\n",
        "}\n",
        "missing = [c for c in need if c not in df_raw_enriched.columns]\n",
        "assert not missing, f\"M4 incomplete; missing: {missing}\"\n",
        "\n",
        "# (Optional) sanity print\n",
        "head_idx = (\n",
        "    df_raw_enriched.sort_values([\"appid\",\"country\",\"timestamp\"], kind=\"mergesort\")\n",
        "                   .groupby([\"appid\",\"country\"]).head(1).index\n",
        ")\n",
        "print(\"First-row base_change AFTER recompute:\",\n",
        "      int(df_raw_enriched.loc[head_idx, \"is_base_change\"].sum()))"
      ],
      "metadata": {
        "id": "f0s05XXv7eaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ae4359d-427e-4181-b31e-ca7b08026dc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[M4] loaded release_dim.parquet: (513, 6)\n",
            "[M4] release_dim ready. shape= (513, 6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3171834628.py:284: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(_flags)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First-row base_change AFTER recompute: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Sanity checks (should all be zero) ---\n",
        "d = df_raw_enriched.sort_values([\"appid\",\"country\",\"timestamp\"], kind=\"mergesort\").copy()\n",
        "\n",
        "# A) group-start currency mismatch should be flagged as at_start\n",
        "head = d.groupby([\"appid\",\"country\"], sort=False).head(1).copy()\n",
        "need = head[\"currency\"].astype(str) != head[\"release_currency\"].astype(str)\n",
        "miss = need & ~head[\"is_currency_change_at_start\"].fillna(False)\n",
        "print(\"start_mismatch_total:\", int(need.sum()))\n",
        "print(\"start_mismatch_not_flagged:\", int(miss.sum()))\n",
        "\n",
        "# B) cross-currency rows must have NaN deltas\n",
        "bad_delta = (d[\"currency\"].astype(str) != d[\"release_currency\"].astype(str)) & d[\"delta_from_release_price\"].notna()\n",
        "print(\"delta_nonNaN_on_mismatch:\", int(bad_delta.sum()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtV2cOCqfAJf",
        "outputId": "8bacd80e-24b5-4072-96c2-240a30b2087f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_mismatch_total: 51\n",
            "start_mismatch_not_flagged: 0\n",
            "delta_nonNaN_on_mismatch: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Focused audit & optional removal for (2) ccy_mismatch and (5) price!=regular & cut==0\n",
        "# - Comments in English\n",
        "# - Safe saves with auto-mkdir; no scraping (only SteamDB URLs are composed)\n",
        "# - Requires you already have df_raw_enriched from Module 4 and the helpers:\n",
        "#   reflag_group_start_currency_mismatch, recompute_is_base_change\n",
        "#   (If not, paste those helpers from your Module 4.)\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def add_steamdb_urls(df):\n",
        "    \"\"\"\n",
        "    Add SteamDB links with lowercase region:\n",
        "      - steamdb_url: app main page\n",
        "      - steamdb_price_url: direct 'Price history' tab\n",
        "    \"\"\"\n",
        "    d = df.copy()\n",
        "    d[\"appid\"] = d[\"appid\"].astype(str)\n",
        "    cc = d[\"country\"].astype(str).str.lower()\n",
        "    base = \"https://steamdb.info/app/\"\n",
        "    d[\"steamdb_url\"] = base + d[\"appid\"] + \"/?cc=\" + cc\n",
        "    d[\"steamdb_price_url\"] = base + d[\"appid\"] + \"/price/?cc=\" + cc\n",
        "    return d\n",
        "\n",
        "# ---------- 0) Utilities ----------\n",
        "def _ensure_outdir():\n",
        "    pref = Path(\"/mnt/data\")\n",
        "    out_dir = pref if pref.exists() else Path.cwd() / \"outputs\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    return out_dir\n",
        "\n",
        "def _first_post_on_or_after_release(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Return the first row ON/AFTER release per (appid,country).\"\"\"\n",
        "    d = df.copy()\n",
        "    d[\"timestamp\"] = pd.to_datetime(d[\"timestamp\"], errors=\"coerce\")\n",
        "    for c in (\"appid\",\"country\",\"currency\",\"release_currency\"):\n",
        "        if c in d.columns:\n",
        "            d[c] = d[c].astype(str)\n",
        "    d = d.sort_values([\"appid\",\"country\",\"timestamp\"], kind=\"mergesort\")\n",
        "    if \"days_since_release\" not in d.columns:\n",
        "        rel_day = pd.to_datetime(d[\"release_time\"], errors=\"coerce\").dt.normalize()\n",
        "        ts_day  = pd.to_datetime(d[\"timestamp\"],    errors=\"coerce\").dt.normalize()\n",
        "        d[\"days_since_release\"] = (ts_day - rel_day).dt.days\n",
        "    mask_post = d[\"days_since_release\"].fillna(0).ge(0)\n",
        "    first_post = d.loc[mask_post].groupby([\"appid\",\"country\"], sort=False).head(1).copy()\n",
        "    # Attach robust SteamDB links (lowercase cc + price tab)\n",
        "    first_post = add_steamdb_urls(first_post)\n",
        "    return first_post\n",
        "\n",
        "def _extract_type_2_and_5(first_post: pd.DataFrame):\n",
        "    \"\"\"Build type-2 (ccy mismatch) and type-5 (price!=regular & cut==0) masks and tables.\"\"\"\n",
        "    tol = 1e-6\n",
        "    same_ccy = first_post[\"currency\"].astype(str) == first_post[\"release_currency\"].astype(str)\n",
        "\n",
        "    # (2) ccy_mismatch at the first post-release row\n",
        "    t2_mask = ~same_ccy\n",
        "    t2 = first_post.loc[t2_mask].copy()\n",
        "\n",
        "    # (5) price != regular but cut==0  (possible hidden sale / mis-typed cut)\n",
        "    #     Only evaluate when price & regular are present.\n",
        "    t5_mask = (\n",
        "        (first_post[\"cut\"].fillna(0) <= 0) &\n",
        "        first_post[\"price\"].notna() & first_post[\"regular\"].notna() &\n",
        "        (first_post[\"price\"] - first_post[\"regular\"]).abs().gt(tol)\n",
        "    )\n",
        "    t5 = first_post.loc[t5_mask].copy()\n",
        "    return t2, t5\n",
        "\n",
        "def drop_first_post_rows(df_enriched: pd.DataFrame,\n",
        "                         first_post_subset: pd.DataFrame,\n",
        "                         confirm_col: str | None = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Drop the first-post rows (per appid,country) specified by `first_post_subset`.\n",
        "    If `confirm_col` is provided, only drop rows where that column is True (allows manual curation).\n",
        "    After dropping, re-flag start-of-group currency mismatch and recompute is_base_change.\n",
        "    \"\"\"\n",
        "    d = df_enriched.copy()\n",
        "    keys = [\"appid\",\"country\",\"timestamp\"]\n",
        "\n",
        "    # Optional manual confirmation column\n",
        "    if confirm_col is not None and confirm_col in first_post_subset.columns:\n",
        "        first_post_subset = first_post_subset[first_post_subset[confirm_col].fillna(False)]\n",
        "\n",
        "    # Prepare dropping keys\n",
        "    drop_keys = first_post_subset[keys].drop_duplicates().copy()\n",
        "    for c in (\"appid\",\"country\"):\n",
        "        drop_keys[c] = drop_keys[c].astype(str)\n",
        "\n",
        "    d[keys[0]] = d[keys[0]].astype(str)\n",
        "    d[keys[1]] = d[keys[1]].astype(str)\n",
        "\n",
        "    # Mark and drop\n",
        "    drop_keys[\"_drop\"] = 1\n",
        "    d2 = d.merge(drop_keys, on=keys, how=\"left\")\n",
        "    removed = int(d2[\"_drop\"].fillna(0).sum())\n",
        "    d2 = d2[d2[\"_drop\"].isna()].drop(columns=[\"_drop\"])\n",
        "\n",
        "    # Post-drop fixes\n",
        "    d2 = reflag_group_start_currency_mismatch(d2)\n",
        "    d2 = recompute_is_base_change(d2, abs_thr=0.10, rel_thr=0.01)\n",
        "\n",
        "    print(f\"Dropped {removed} first-post rows; reflagged start-of-group FX; recomputed base changes.\")\n",
        "    return d2\n",
        "\n",
        "# ---------- 1) Build the audit views for type (2) and (5) ----------\n",
        "first_post = _first_post_on_or_after_release(df_raw_enriched)\n",
        "t2_ccy_mismatch, t5_price_neq_reg_cut0 = _extract_type_2_and_5(first_post)\n",
        "\n",
        "print(f\"(2) ccy_mismatch: {len(t2_ccy_mismatch)} rows\")\n",
        "print(f\"(5) price!=regular & cut==0: {len(t5_price_neq_reg_cut0)} rows\")\n",
        "\n",
        "# Save the two lists for manual audit (adds an optional 'confirm_delete' column placeholder)\n",
        "out_dir = _ensure_outdir()\n",
        "ts = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "t2_path = out_dir / f\"audit_type2_ccy_mismatch_first_post_{ts}.csv\"\n",
        "t5_path = out_dir / f\"audit_type5_price_ne_reg_cut0_first_post_{ts}.csv\"\n",
        "\n",
        "if \"confirm_delete\" not in t2_ccy_mismatch.columns:\n",
        "    t2_ccy_mismatch[\"confirm_delete\"] = False  # you can edit this CSV and set True for rows you want to remove\n",
        "\n",
        "cols_keep = [c for c in [\n",
        "    \"appid\",\"country\",\"timestamp\",\"currency\",\"release_currency\",\n",
        "    \"price\",\"regular\",\"release_price\",\"cut\",\n",
        "    \"is_sale\",\"is_currency_change\",\"is_currency_change_at_start\",\"is_base_change\",\n",
        "    \"is_release_day\",\"days_since_release\",\"steamdb_url\",\"confirm_delete\"\n",
        "] if c in t2_ccy_mismatch.columns]\n",
        "\n",
        "t2_ccy_mismatch.to_csv(t2_path, index=False, columns=cols_keep)\n",
        "t5_price_neq_reg_cut0.to_csv(t5_path, index=False)\n",
        "\n",
        "print(\"Saved audit CSVs:\")\n",
        "print(\" -\", t2_path)\n",
        "print(\" -\", t5_path)\n",
        "\n",
        "# ---------- 2) OPTION A: delete ALL type-2 rows immediately (no manual confirmation) ----------\n",
        "# WARNING: this removes the first-post row for those (appid,country); use only if you are sure.\n",
        "# df_clean = drop_first_post_rows(df_raw_enriched, t2_ccy_mismatch, confirm_col=None)\n",
        "\n",
        "# ---------- 3) OPTION B: manual confirm, then delete only confirmed rows ----------\n",
        "# After you edit the CSV (set confirm_delete=True where you verified mismatch on SteamDB),\n",
        "# load it back and call drop_first_post_rows with confirm_col=\"confirm_delete\".\n",
        "# confirmed = pd.read_csv(t2_path, parse_dates=[\"timestamp\"])\n",
        "# df_clean = drop_first_post_rows(df_raw_enriched, confirmed, confirm_col=\"confirm_delete\")\n",
        "\n",
        "# ---------- 4) (optional) persist the cleaned table ----------\n",
        "# out_clean = out_dir / f\"steam_events_raw_enriched_cleaned_drop_t2_{ts}.csv\"\n",
        "# df_clean.to_csv(out_clean, index=False)\n",
        "# print(\"Cleaned table saved to:\", out_clean)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIddG_cDlfqz",
        "outputId": "59fba5ed-d765-46fc-dfac-f63c63db7be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2) ccy_mismatch: 51 rows\n",
            "(5) price!=regular & cut==0: 0 rows\n",
            "Saved audit CSVs:\n",
            " - /content/outputs/audit_type2_ccy_mismatch_first_post_20251107_062905.csv\n",
            " - /content/outputs/audit_type5_price_ne_reg_cut0_first_post_20251107_062905.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PRE-REBASE SANITY CHECK (READ-ONLY) ---\n",
        "# What countries/currencies would need FX, based on \"post-release & currency!=release_currency\"?\n",
        "\n",
        "d = df_raw_enriched.copy()\n",
        "post = d['days_since_release'].fillna(0).ge(0)\n",
        "mismatch = d['currency'].astype(str) != d['release_currency'].astype(str)\n",
        "\n",
        "need_fx_view = d.loc[post & mismatch, ['appid','country','currency','release_currency']]\n",
        "\n",
        "print(\"Rows needing FX by country:\")\n",
        "print(need_fx_view['country'].value_counts().head(20))\n",
        "\n",
        "print(\"\\nRelease_currency among rows needing FX:\")\n",
        "print(need_fx_view['release_currency'].value_counts().head(10))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED5A7mRQQqLX",
        "outputId": "843f471b-bcbc-4155-9907-48f55de9b936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows needing FX by country:\n",
            "country\n",
            "PL    1598\n",
            "TR    1026\n",
            "JP      80\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Release_currency among rows needing FX:\n",
            "release_currency\n",
            "PLN    1598\n",
            "USD    1026\n",
            "JPY      80\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ---------- you already have this; keep it as-is ----------\n",
        "def detect_leading_fx_need(df: pd.DataFrame, scope={\"TR\",\"PL\",\"JP\"}) -> dict:\n",
        "    d = df.copy()\n",
        "    d[\"timestamp\"] = pd.to_datetime(d[\"timestamp\"], errors=\"coerce\")\n",
        "    d[\"release_time\"] = pd.to_datetime(d[\"release_time\"], errors=\"coerce\")\n",
        "    d[\"ts_day\"] = d[\"timestamp\"].dt.normalize()\n",
        "    d[\"rel_day\"] = d[\"release_time\"].dt.normalize()\n",
        "    d[\"days_since_release\"] = (d[\"ts_day\"] - d[\"rel_day\"]).dt.days\n",
        "    for c in (\"appid\",\"country\",\"currency\",\"release_currency\"):\n",
        "        d[c] = d[c].astype(str)\n",
        "    if scope is not None:\n",
        "        d = d[d[\"country\"].isin(scope)].copy()\n",
        "\n",
        "    gcols = [\"appid\",\"country\"]\n",
        "    d = d.sort_values(gcols + [\"timestamp\"], kind=\"mergesort\")\n",
        "\n",
        "    post = d[\"days_since_release\"].fillna(0).ge(0)\n",
        "    mismatch = d[\"currency\"].astype(str) != d[\"release_currency\"].astype(str)\n",
        "    match    = ~mismatch\n",
        "\n",
        "    post_rank     = post.groupby(d[gcols].apply(tuple, axis=1)).cumsum()\n",
        "    is_first_post = (post & (post_rank == 1))\n",
        "    first_post_mismatch = (is_first_post & mismatch)\n",
        "    grp_key = d[gcols].apply(tuple, axis=1)\n",
        "    needs_group = first_post_mismatch.groupby(grp_key).transform(\"max\").astype(bool)\n",
        "\n",
        "    seen_match_after = ((post & match).groupby(grp_key).cumsum() > 0)\n",
        "    leading_seg_mask = post & needs_group & (~seen_match_after)\n",
        "\n",
        "    out = {}\n",
        "    out[\"rows_by_country\"]   = d.loc[leading_seg_mask, \"country\"].value_counts().to_dict()\n",
        "    out[\"groups_by_country\"] = (d.loc[leading_seg_mask, gcols]\n",
        "                                  .drop_duplicates()\n",
        "                                  .groupby(\"country\").size().to_dict())\n",
        "    out[\"impacted_groups_df\"] = (d.loc[leading_seg_mask, gcols + [\"release_currency\"]]\n",
        "                                   .assign(one=1)\n",
        "                                   .groupby(gcols + [\"release_currency\"], as_index=False)[\"one\"].sum()\n",
        "                                   .rename(columns={\"one\":\"rows_in_leading_segment\"})\n",
        "                                   .sort_values([\"country\",\"rows_in_leading_segment\",\"appid\"],\n",
        "                                                ascending=[True,False,True]))\n",
        "    out[\"jp_sample_df\"] = out[\"impacted_groups_df\"][out[\"impacted_groups_df\"][\"country\"]==\"JP\"].head(25)\n",
        "    out[\"leading_mask\"] = leading_seg_mask\n",
        "    return out\n",
        "\n",
        "# --- FIXED: build daily FX over the FULL events date span ---\n",
        "def build_fx_daily_fixed(ffx: pd.DataFrame,\n",
        "                         events_df: pd.DataFrame,\n",
        "                         two_sided_fill: bool = True,\n",
        "                         alias_map: dict | None = None):\n",
        "    \"\"\"\n",
        "    Make a COMPLETE daily USD-per-unit table for all currencies across the FULL\n",
        "    [min(ts_day) .. max(ts_day)] span seen in events_df.\n",
        "    This fixes the 'can't bfill earlier than the currency's own first date' problem.\n",
        "    Returns: (fx_daily, report)\n",
        "      - fx_daily: ['ccy','date','usd'] with no gaps if that ccy has at least 1 snapshot\n",
        "      - report: {'span':(min_date,max_date), 'ccy_with_no_snapshot': [...]}\n",
        "    \"\"\"\n",
        "    fx = ffx.copy()\n",
        "    fx[\"date\"] = pd.to_datetime(fx[\"date\"], errors=\"coerce\")\n",
        "    fx[\"ccy\"]  = fx[\"ccy\"].astype(str).str.upper().str.strip()\n",
        "    if alias_map:\n",
        "        fx[\"ccy\"] = fx[\"ccy\"].replace(alias_map)\n",
        "\n",
        "    fx = fx.dropna(subset=[\"date\",\"ccy\",\"usd_per_unit\"]).sort_values([\"ccy\",\"date\"], kind=\"mergesort\")\n",
        "\n",
        "    # global span from events_df\n",
        "    ev = events_df.copy()\n",
        "    ev[\"timestamp\"] = pd.to_datetime(ev[\"timestamp\"], errors=\"coerce\")\n",
        "    span_min = ev[\"timestamp\"].min().normalize()\n",
        "    span_max = ev[\"timestamp\"].max().normalize()\n",
        "    full_idx = pd.date_range(span_min, span_max, freq=\"D\")\n",
        "\n",
        "    # build per-ccy on the FULL index\n",
        "    chunks = []\n",
        "    ccy_with_no_snapshot = []\n",
        "    for ccy, g in fx.groupby(\"ccy\", sort=False):\n",
        "        if g.empty:\n",
        "            ccy_with_no_snapshot.append(ccy)\n",
        "            continue\n",
        "        s = g.set_index(\"date\")[\"usd_per_unit\"].sort_index()\n",
        "        s = s.reindex(full_idx)             # <-- extend to FULL span\n",
        "        s = s.ffill()\n",
        "        if two_sided_fill:\n",
        "            s = s.bfill()\n",
        "        df_ccy = pd.DataFrame({\"ccy\": ccy, \"date\": s.index, \"usd\": s.values})\n",
        "        # If even after ffill+bfill it's all NaN, mark as no snapshot\n",
        "        if not df_ccy[\"usd\"].notna().any():\n",
        "            ccy_with_no_snapshot.append(ccy)\n",
        "        chunks.append(df_ccy)\n",
        "\n",
        "    fx_daily = pd.concat(chunks, ignore_index=True) if chunks else pd.DataFrame(columns=[\"ccy\",\"date\",\"usd\"])\n",
        "    report = {\"span\": (span_min, span_max), \"ccy_with_no_snapshot\": sorted(set(ccy_with_no_snapshot))}\n",
        "    return fx_daily, report\n",
        "\n",
        "    return fx_daily\n",
        "\n",
        "# --- Strict rebase using a PREBUILT fx_daily (no manual FX editing) ---\n",
        "def fx_rebase_on_detected_prebuilt(df_enriched: pd.DataFrame,\n",
        "                                   fx_daily: pd.DataFrame,\n",
        "                                   detected: dict,\n",
        "                                   scope_countries: set[str] | None = {\"TR\",\"PL\",\"JP\"}) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Same semantics as your fx_rebase_on_detected, but consumes a prebuilt fx_daily ['ccy','date','usd'].\n",
        "    \"\"\"\n",
        "    d = df_enriched.copy()\n",
        "    d[\"timestamp\"] = pd.to_datetime(d[\"timestamp\"], errors=\"coerce\")\n",
        "    d[\"release_time\"] = pd.to_datetime(d[\"release_time\"], errors=\"coerce\")\n",
        "    d[\"ts_day\"] = d[\"timestamp\"].dt.normalize()\n",
        "    if \"days_since_release\" not in d.columns:\n",
        "        d[\"days_since_release\"] = (d[\"ts_day\"] - d[\"release_time\"].dt.normalize()).dt.days\n",
        "    for c in (\"appid\",\"country\",\"currency\",\"release_currency\"):\n",
        "        d[c] = d[c].astype(str).str.upper().str.strip()\n",
        "\n",
        "    mask = detected[\"leading_mask\"]\n",
        "    if getattr(mask, \"index\", None) is not d.index:\n",
        "        mask = mask.reindex(d.index, fill_value=False)\n",
        "    if scope_countries is not None:\n",
        "        mask = mask & d[\"country\"].isin(scope_countries)\n",
        "\n",
        "    if \"note_day0_fx_needed\" not in d.columns:\n",
        "        d[\"note_day0_fx_needed\"] = False\n",
        "    if \"note_day0_fx_rebased\" not in d.columns:\n",
        "        d[\"note_day0_fx_rebased\"] = False\n",
        "    d.loc[mask, \"note_day0_fx_needed\"] = True\n",
        "\n",
        "    if not bool(mask.any()):\n",
        "        print(\"[Info] No rows to rebase under current scope.\")\n",
        "        return d\n",
        "\n",
        "    # Map FX from prebuilt grid\n",
        "    left = d.loc[mask].copy()\n",
        "    left[\"_row_id\"] = left.index\n",
        "    ev = fx_daily.rename(columns={\"ccy\":\"currency\",\"usd\":\"usd_per_event\"})\n",
        "    rl = fx_daily.rename(columns={\"ccy\":\"release_currency\",\"usd\":\"usd_per_release\"})\n",
        "    left = (left.merge(ev, how=\"left\", left_on=[\"currency\",\"ts_day\"], right_on=[\"currency\",\"date\"]).drop(columns=[\"date\"])\n",
        "                 .merge(rl, how=\"left\", left_on=[\"release_currency\",\"ts_day\"], right_on=[\"release_currency\",\"date\"]).drop(columns=[\"date\"]))\n",
        "\n",
        "    ok = left[\"usd_per_event\"].gt(0) & left[\"usd_per_release\"].gt(0)\n",
        "    left_ok = left.loc[ok].copy()\n",
        "\n",
        "    ratio = left_ok[\"usd_per_event\"] / left_ok[\"usd_per_release\"]\n",
        "    for col in (\"price\",\"regular\"):\n",
        "        if col in left_ok.columns:\n",
        "            left_ok[col] = pd.to_numeric(left_ok[col], errors=\"coerce\") * ratio\n",
        "\n",
        "    left_ok[\"currency\"] = left_ok[\"release_currency\"]\n",
        "    left_ok[\"note_day0_fx_rebased\"] = True\n",
        "\n",
        "    row_ids = left_ok[\"_row_id\"].to_numpy()\n",
        "    d.loc[row_ids, [\"price\",\"regular\",\"currency\",\"note_day0_fx_rebased\"]] = \\\n",
        "        left_ok[[\"price\",\"regular\",\"currency\",\"note_day0_fx_rebased\"]].to_numpy()\n",
        "\n",
        "    # light relabel + sanity (same as你的版本)\n",
        "    d[\"is_sale\"] = (pd.to_numeric(d.get(\"cut\"), errors=\"coerce\").fillna(0) > 0) | (\n",
        "        d[\"price\"].notna() & d[\"regular\"].notna() & (d[\"price\"] < d[\"regular\"])\n",
        "    )\n",
        "    same_ccy = d[\"currency\"] == d[\"release_currency\"]\n",
        "    d[\"delta_from_release_price\"] = np.where(same_ccy,\n",
        "        pd.to_numeric(d[\"price\"], errors=\"coerce\") - pd.to_numeric(d[\"release_price\"], errors=\"coerce\"),\n",
        "        np.nan)\n",
        "    d[\"delta_pct_from_release\"] = np.where(same_ccy & pd.to_numeric(d[\"release_price\"], errors=\"coerce\").gt(0),\n",
        "        pd.to_numeric(d[\"price\"], errors=\"coerce\") / pd.to_numeric(d[\"release_price\"], errors=\"coerce\") - 1.0,\n",
        "        np.nan)\n",
        "\n",
        "    d_chk = d.sort_values([\"appid\",\"country\",\"timestamp\"], kind=\"mergesort\")\n",
        "    fp = d_chk[d_chk[\"days_since_release\"].fillna(0).ge(0)].groupby([\"appid\",\"country\"], sort=False).head(1)\n",
        "    ok_first = bool((fp[\"currency\"] == fp[\"release_currency\"]).all())\n",
        "    print(f\"[Sanity] first-post matches release_currency: {ok_first}\")\n",
        "    print(f\"[Sanity] rebased rows: {len(row_ids)} | changed countries: {dict(d.loc[row_ids, 'country'].value_counts())}\")\n",
        "    return d\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZXfiEUZwMUvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect leading window as before\n",
        "\n",
        "rows = [\n",
        "    (\"2023-08-03\",\"USD\",1.0000), (\"2023-08-03\",\"EUR\",1.1000), (\"2023-08-03\",\"PLN\",0.2500), (\"2023-08-03\",\"TRY\",0.0350), (\"2023-08-03\",\"JPY\",0.0070),\n",
        "    (\"2024-06-01\",\"USD\",1.0000), (\"2024-06-01\",\"EUR\",1.0800), (\"2024-06-01\",\"PLN\",0.2600), (\"2024-06-01\",\"TRY\",0.0300), (\"2024-06-01\",\"JPY\",0.0068),\n",
        "    (\"2025-06-01\",\"USD\",1.0000), (\"2025-06-01\",\"EUR\",1.0700), (\"2025-06-01\",\"PLN\",0.2550), (\"2025-06-01\",\"TRY\",0.0280), (\"2025-06-01\",\"JPY\",0.0069),\n",
        "]\n",
        "fx_df = pd.DataFrame(rows, columns=[\"date\",\"ccy\",\"usd_per_unit\"])\n",
        "fx_df[\"date\"] = pd.to_datetime(fx_df[\"date\"])\n",
        "scope = {\"TR\",\"PL\",\"JP\"}\n",
        "fx_need = detect_leading_fx_need(df_raw_enriched, scope=scope)\n",
        "\n",
        "# Prebuild FX DAILY over FULL event span (this is the key fix)\n",
        "fx_daily_fixed, fx_report = build_fx_daily_fixed(fx_df, df_raw_enriched, two_sided_fill=True)\n",
        "print(\"[FX span]\", fx_report[\"span\"])\n",
        "print(\"[Currencies with NO snapshot at all]\", fx_report[\"ccy_with_no_snapshot\"])\n",
        "\n",
        "# Rebase using the prebuilt grid (no manual edits)\n",
        "df_rebased = fx_rebase_on_detected_prebuilt(\n",
        "    df_enriched=df_raw_enriched,\n",
        "    fx_daily=fx_daily_fixed,\n",
        "    detected=fx_need,\n",
        "    scope_countries=scope\n",
        ")\n",
        "\n",
        "# Save with your fixed name (no timestamp)\n",
        "df_rebased.to_csv(\"steam_events_raw_enriched_rebased_fx.csv\", index=False)\n",
        "print(\"[SAVE] -> steam_events_raw_enriched_rebased_fx.csv\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDVmWynBwvgF",
        "outputId": "b1a9e7fc-9f32-4c5d-dca6-ed8981f99d55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FX span] (Timestamp('2015-11-06 00:00:00'), Timestamp('2025-11-06 00:00:00'))\n",
            "[Currencies with NO snapshot at all] []\n",
            "[Sanity] first-post matches release_currency: True\n",
            "[Sanity] rebased rows: 2704 | changed countries: {'PL': np.int64(1598), 'TR': np.int64(1026), 'JP': np.int64(80)}\n",
            "[SAVE] -> steam_events_raw_enriched_rebased_fx.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify where 3354750 appears\n",
        "import pandas as pd\n",
        "\n",
        "appid = \"3354750\"\n",
        "sellers = pd.read_csv(\"steam_top_sellers.csv\")\n",
        "prices  = pd.read_csv(\"steam_prices.csv\")\n",
        "events  = pd.read_csv(\"steam_events_raw.csv\")  # or use your in-memory df_events\n",
        "\n",
        "print(\"in sellers:\", (sellers[\"appid\"].astype(str) == appid).any())\n",
        "print(\"in prices:\",  (prices[\"appid\"].astype(str)  == appid).any())\n",
        "print(\"in events:\",  (events[\"appid\"].astype(str)  == appid).any())\n"
      ],
      "metadata": {
        "id": "5n96gOtBskBm",
        "outputId": "fd67ddcc-e1ab-44aa-b03f-67534656b6b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in sellers: True\n",
            "in prices: False\n",
            "in events: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Why is appid 3354750 missing? Quick triage ---\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "appid = \"3354750\"\n",
        "\n",
        "# Load what you already produced\n",
        "df_events  = pd.read_csv(\"steam_events_raw.csv\")                 # M3 output\n",
        "df_sellers = pd.read_csv(\"steam_top_sellers.csv\")                # seed list\n",
        "# If df_gid exists in memory from M2, use it; otherwise load if you saved it.\n",
        "# df_gid should have columns: ['appid','gid']\n",
        "# If you never saved df_gid, re-run your M2 lookup for this single app.\n",
        "\n",
        "print(\"in sellers:\", (df_sellers[\"appid\"].astype(str) == appid).any())\n",
        "print(\"in events :\", (df_events[\"appid\"].astype(str)  == appid).any())\n",
        "\n",
        "try:\n",
        "    df_gid\n",
        "    has_map = (df_gid[\"appid\"].astype(str) == appid).any()\n",
        "    print(\"in gid map:\", has_map)\n",
        "    if has_map:\n",
        "        print(df_gid.loc[df_gid[\"appid\"].astype(str)==appid])\n",
        "    else:\n",
        "        print(\"Likely cause: /lookup did not return a gid for this appid.\")\n",
        "except NameError:\n",
        "    print(\"df_gid not in memory. If needed, re-run your M2 lookup for this appid to confirm mapping.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tm9YN0UmtDhM",
        "outputId": "e397f624-9e74-49ea-a953-b421ca4e32e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in sellers: True\n",
            "in events : False\n",
            "in gid map: True\n",
            "      appid                                   gid\n",
            "39  3354750  018d937f-4b80-737f-9a8a-409b0056f110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Robust one-off probe for a single GID (handles dict OR list JSON shapes) ---\n",
        "\n",
        "import os, requests, pandas as pd\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "ITAD_BASE = \"https://api.isthereanydeal.com\"\n",
        "ITAD_API_KEY = os.environ.get(\"ITAD_API_KEY\")  # make sure it's set\n",
        "STEAM_SHOP_ID = 61\n",
        "\n",
        "def _extract_events_list(data):\n",
        "    \"\"\"Return a list of events regardless of JSON envelope shape.\"\"\"\n",
        "    if isinstance(data, list):\n",
        "        return data\n",
        "    if isinstance(data, dict):\n",
        "        for k in (\"data\", \"history\", \"list\", \"events\"):\n",
        "            v = data.get(k)\n",
        "            if isinstance(v, list):\n",
        "                return v\n",
        "        return []\n",
        "    return []\n",
        "\n",
        "def probe_history(gid: str, countries=(\"US\",\"GB\",\"DE\",\"JP\",\"CN\"), days=365*12) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Call /games/history/v2 for several countries + a no-country fallback.\n",
        "    Summarize by shop and show a small head.\n",
        "    \"\"\"\n",
        "    assert ITAD_API_KEY, \"ITAD_API_KEY is not set in environment.\"\n",
        "    url = f\"{ITAD_BASE}/games/history/v2\"\n",
        "    since = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat(timespec=\"seconds\").replace(\"+00:00\",\"Z\")\n",
        "\n",
        "    frames = []\n",
        "    for cc in list(countries) + [None]:  # final call without country\n",
        "        params = {\"key\": ITAD_API_KEY, \"id\": gid, \"since\": since}\n",
        "        if cc:\n",
        "            params[\"country\"] = cc.lower()\n",
        "        r = requests.get(url, params=params, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        events = _extract_events_list(r.json())\n",
        "        if not events:\n",
        "            continue\n",
        "\n",
        "        rows = []\n",
        "        for e in events:\n",
        "            shop = e.get(\"shop\") or {}\n",
        "            deal = e.get(\"deal\") or {}\n",
        "            price = (deal.get(\"price\") or {})  # dict or None\n",
        "            regular = (deal.get(\"regular\") or {})\n",
        "            rows.append({\n",
        "                \"ts\": e.get(\"timestamp\") or e.get(\"recorded\") or e.get(\"time\"),\n",
        "                \"shop_id\": int(shop.get(\"id\") or 0),\n",
        "                \"shop_name\": shop.get(\"name\"),\n",
        "                \"ccy\": price.get(\"currency\"),\n",
        "                # use amountInt if present, otherwise amount (already float)\n",
        "                \"price_amt\": (price.get(\"amountInt\")/100.0 if price.get(\"amountInt\") is not None else price.get(\"amount\")),\n",
        "                \"regular_amt\": (regular.get(\"amountInt\")/100.0 if regular.get(\"amountInt\") is not None else regular.get(\"amount\")),\n",
        "                \"cut\": deal.get(\"cut\"),\n",
        "                \"query_country\": cc or \"no-country\",\n",
        "            })\n",
        "        frames.append(pd.DataFrame(rows))\n",
        "\n",
        "    out = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
        "    if out.empty:\n",
        "        print(\"No events returned from API for this gid.\")\n",
        "        return out\n",
        "\n",
        "    # Normalize and summarize\n",
        "    out[\"shop_id\"] = pd.to_numeric(out[\"shop_id\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    steam_cnt = int((out[\"shop_id\"] == STEAM_SHOP_ID).sum())\n",
        "    print(f\"Total events: {len(out)} | Steam events: {steam_cnt}\")\n",
        "    print(\"By shop_id:\", out[\"shop_id\"].value_counts().to_dict())\n",
        "    print(out.head(10))\n",
        "    return out\n",
        "\n",
        "\n",
        "# Example: pass the gid you printed from your df_gid table\n",
        "probe_history(\"018d937f-4b80-737f-9a8a-409b0056f110\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "Kh20Nv35tcjA",
        "outputId": "b5e531ce-3f8f-4217-c2c6-986256bb13b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total events: 11 | Steam events: 0\n",
            "By shop_id: {16: 6, 52: 5}\n",
            "                          ts  shop_id        shop_name  ccy  price_amt  regular_amt  cut query_country\n",
            "0  2025-09-16T19:48:25+02:00       16  Epic Game Store  USD        0.0          0.0    0            US\n",
            "1  2025-09-16T19:11:10+02:00       52         EA Store  USD        0.0          0.0    0            US\n",
            "2  2025-09-16T20:02:32+02:00       16  Epic Game Store  GBP        0.0          0.0    0            GB\n",
            "3  2025-09-16T19:50:37+02:00       52         EA Store  GBP        0.0          0.0    0            GB\n",
            "4  2025-09-16T20:16:01+02:00       16  Epic Game Store  EUR        0.0          0.0    0            DE\n",
            "5  2025-09-16T19:11:19+02:00       52         EA Store  EUR        0.0          0.0    0            DE\n",
            "6  2025-09-16T19:51:46+02:00       52         EA Store  JPY        0.0          0.0    0            JP\n",
            "7  2025-09-16T19:50:49+02:00       16  Epic Game Store  JPY        0.0          0.0    0            JP\n",
            "8  2025-09-16T19:55:57+02:00       16  Epic Game Store  CNY        0.0          0.0    0            CN\n",
            "9  2025-09-16T19:48:25+02:00       16  Epic Game Store  USD        0.0          0.0    0    no-country\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           ts  shop_id        shop_name  ccy  price_amt  regular_amt  cut query_country\n",
              "0   2025-09-16T19:48:25+02:00       16  Epic Game Store  USD        0.0          0.0    0            US\n",
              "1   2025-09-16T19:11:10+02:00       52         EA Store  USD        0.0          0.0    0            US\n",
              "2   2025-09-16T20:02:32+02:00       16  Epic Game Store  GBP        0.0          0.0    0            GB\n",
              "3   2025-09-16T19:50:37+02:00       52         EA Store  GBP        0.0          0.0    0            GB\n",
              "4   2025-09-16T20:16:01+02:00       16  Epic Game Store  EUR        0.0          0.0    0            DE\n",
              "..                        ...      ...              ...  ...        ...          ...  ...           ...\n",
              "6   2025-09-16T19:51:46+02:00       52         EA Store  JPY        0.0          0.0    0            JP\n",
              "7   2025-09-16T19:50:49+02:00       16  Epic Game Store  JPY        0.0          0.0    0            JP\n",
              "8   2025-09-16T19:55:57+02:00       16  Epic Game Store  CNY        0.0          0.0    0            CN\n",
              "9   2025-09-16T19:48:25+02:00       16  Epic Game Store  USD        0.0          0.0    0    no-country\n",
              "10  2025-09-16T19:11:10+02:00       52         EA Store  USD        0.0          0.0    0    no-country\n",
              "\n",
              "[11 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-548c70eb-c2df-4fc9-b02f-89cd634930b8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ts</th>\n",
              "      <th>shop_id</th>\n",
              "      <th>shop_name</th>\n",
              "      <th>ccy</th>\n",
              "      <th>price_amt</th>\n",
              "      <th>regular_amt</th>\n",
              "      <th>cut</th>\n",
              "      <th>query_country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-09-16T19:48:25+02:00</td>\n",
              "      <td>16</td>\n",
              "      <td>Epic Game Store</td>\n",
              "      <td>USD</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>US</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-09-16T19:11:10+02:00</td>\n",
              "      <td>52</td>\n",
              "      <td>EA Store</td>\n",
              "      <td>USD</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>US</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-09-16T20:02:32+02:00</td>\n",
              "      <td>16</td>\n",
              "      <td>Epic Game Store</td>\n",
              "      <td>GBP</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>GB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-09-16T19:50:37+02:00</td>\n",
              "      <td>52</td>\n",
              "      <td>EA Store</td>\n",
              "      <td>GBP</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>GB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-09-16T20:16:01+02:00</td>\n",
              "      <td>16</td>\n",
              "      <td>Epic Game Store</td>\n",
              "      <td>EUR</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>DE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2025-09-16T19:51:46+02:00</td>\n",
              "      <td>52</td>\n",
              "      <td>EA Store</td>\n",
              "      <td>JPY</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>JP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2025-09-16T19:50:49+02:00</td>\n",
              "      <td>16</td>\n",
              "      <td>Epic Game Store</td>\n",
              "      <td>JPY</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>JP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2025-09-16T19:55:57+02:00</td>\n",
              "      <td>16</td>\n",
              "      <td>Epic Game Store</td>\n",
              "      <td>CNY</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>CN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2025-09-16T19:48:25+02:00</td>\n",
              "      <td>16</td>\n",
              "      <td>Epic Game Store</td>\n",
              "      <td>USD</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>no-country</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2025-09-16T19:11:10+02:00</td>\n",
              "      <td>52</td>\n",
              "      <td>EA Store</td>\n",
              "      <td>USD</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>no-country</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11 rows × 8 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-548c70eb-c2df-4fc9-b02f-89cd634930b8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-548c70eb-c2df-4fc9-b02f-89cd634930b8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-548c70eb-c2df-4fc9-b02f-89cd634930b8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-adda56c9-2088-4dc7-9adb-887431e92bf2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-adda56c9-2088-4dc7-9adb-887431e92bf2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-adda56c9-2088-4dc7-9adb-887431e92bf2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"probe_history(\\\"018d937f-4b80-737f-9a8a-409b0056f110\\\")\",\n  \"rows\": 11,\n  \"fields\": [\n    {\n      \"column\": \"ts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"2025-09-16T19:50:49+02:00\",\n          \"2025-09-16T19:11:10+02:00\",\n          \"2025-09-16T19:11:19+02:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"shop_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18,\n        \"min\": 16,\n        \"max\": 52,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          52,\n          16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"shop_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"EA Store\",\n          \"Epic Game Store\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ccy\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"GBP\",\n          \"CNY\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"price_amt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"regular_amt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cut\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"query_country\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"US\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ---------- Utility: coalesce duplicates by (appid, country, timestamp) ----------\n",
        "def coalesce_by_key(df: pd.DataFrame,\n",
        "                    gcols=(\"appid\",\"country\"),\n",
        "                    time_col=\"timestamp\",\n",
        "                    keep=\"last\") -> pd.DataFrame:\n",
        "    \"\"\"Keep a single row per (appid,country,timestamp) to avoid duplicate-start joins.\"\"\"\n",
        "    d = df.copy()\n",
        "    d[time_col] = pd.to_datetime(d[time_col], errors=\"coerce\")\n",
        "    for c in gcols:\n",
        "        d[c] = d[c].astype(str)\n",
        "    d = d.sort_values(list(gcols) + [time_col], kind=\"mergesort\")\n",
        "    return d.drop_duplicates(subset=[*gcols, time_col], keep=keep)\n",
        "\n",
        "# ---------- Tolerant sale_state (unchanged) ----------\n",
        "def compute_sale_state(df: pd.DataFrame,\n",
        "                       tol_abs: float = 1.0,\n",
        "                       tol_pct: float = 0.01,\n",
        "                       price_col: str = \"price\",\n",
        "                       reg_col: str = \"regular\") -> pd.Series:\n",
        "    \"\"\"Mark 'in sale' if (regular - price) beats BOTH absolute and percentage thresholds; let cut>0 vote.\"\"\"\n",
        "    p = pd.to_numeric(df[price_col], errors=\"coerce\")\n",
        "    r = pd.to_numeric(df[reg_col], errors=\"coerce\")\n",
        "    ok = p.notna() & r.notna() & (r > 0)\n",
        "    diff = r - p\n",
        "    thr  = np.maximum(tol_abs, tol_pct * r)\n",
        "    sale = ok & (diff > thr)\n",
        "    if \"cut\" in df.columns:\n",
        "        sale = sale | (pd.to_numeric(df[\"cut\"], errors=\"coerce\").fillna(0) > 0)\n",
        "    return sale.fillna(False)\n",
        "\n",
        "# ---------- FIXED: Build SALE episodes (use group.name for keys) ----------\n",
        "# --- FIXED: use numpy timedelta division to compute days ---\n",
        "def build_episodes_with_end_event(df: pd.DataFrame,\n",
        "                                  gcols=(\"appid\",\"country\"),\n",
        "                                  time_col=\"timestamp\",\n",
        "                                  tol_abs: float = 1.0,\n",
        "                                  tol_pct: float = 0.01) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Scan tolerant sale_state and produce:\n",
        "      start_ts, end_last_sale_ts, end_event_ts,\n",
        "      duration_days_in_sale, duration_days_inclusive\n",
        "    NOTE: durations are computed via numpy timedelta division to avoid .total_seconds() on numpy.timedelta64.\n",
        "    \"\"\"\n",
        "    d = df.copy()\n",
        "    d[time_col] = pd.to_datetime(d[time_col], errors=\"coerce\")\n",
        "    d[gcols[0]] = d[gcols[0]].astype(str).str.upper().str.strip()\n",
        "    d[gcols[1]] = d[gcols[1]].astype(str).str.upper().str.strip()\n",
        "    d = d.sort_values(list(gcols)+[time_col], kind=\"mergesort\")\n",
        "\n",
        "    d[\"sale_state\"] = compute_sale_state(d, tol_abs=tol_abs, tol_pct=tol_pct)\n",
        "\n",
        "    out = []\n",
        "    for (a,c), g in d.groupby(list(gcols), sort=False):\n",
        "        ss = g[\"sale_state\"].to_numpy()\n",
        "        # ensure numpy datetime64[ns] so that differences are numpy.timedelta64\n",
        "        ts = g[time_col].to_numpy(dtype=\"datetime64[ns]\")\n",
        "\n",
        "        in_sale, start_idx = False, None\n",
        "        for i in range(len(g)):\n",
        "            if (not in_sale) and ss[i]:\n",
        "                in_sale, start_idx = True, i\n",
        "            elif in_sale and (not ss[i]):\n",
        "                end_last_sale_idx = i - 1\n",
        "                # --- key change: compute days via numpy timedelta division ---\n",
        "                dur_in_sale = float((ts[end_last_sale_idx] - ts[start_idx]) / np.timedelta64(1, \"D\"))\n",
        "                dur_inclusive = float((ts[i] - ts[start_idx]) / np.timedelta64(1, \"D\"))\n",
        "                out.append({\n",
        "                    gcols[0]: a, gcols[1]: c,\n",
        "                    \"start_ts\": ts[start_idx].astype(\"datetime64[ns]\").astype(\"datetime64[ms]\"),\n",
        "                    \"end_last_sale_ts\": ts[end_last_sale_idx].astype(\"datetime64[ns]\").astype(\"datetime64[ms]\"),\n",
        "                    \"end_event_ts\": ts[i].astype(\"datetime64[ns]\").astype(\"datetime64[ms]\"),\n",
        "                    \"duration_days_in_sale\": dur_in_sale,\n",
        "                    \"duration_days_inclusive\": dur_inclusive,\n",
        "                })\n",
        "                in_sale, start_idx = False, None\n",
        "        if in_sale:\n",
        "            end_last_sale_idx = len(g) - 1\n",
        "            dur_in_sale = float((ts[end_last_sale_idx] - ts[start_idx]) / np.timedelta64(1, \"D\"))\n",
        "            out.append({\n",
        "                gcols[0]: a, gcols[1]: c,\n",
        "                \"start_ts\": ts[start_idx].astype(\"datetime64[ns]\").astype(\"datetime64[ms]\"),\n",
        "                \"end_last_sale_ts\": ts[end_last_sale_idx].astype(\"datetime64[ns]\").astype(\"datetime64[ms]\"),\n",
        "                \"end_event_ts\": pd.NaT,\n",
        "                \"duration_days_in_sale\": dur_in_sale,\n",
        "                \"duration_days_inclusive\": dur_in_sale,  # fallback when no explicit end row\n",
        "            })\n",
        "\n",
        "    episodes = pd.DataFrame(out).sort_values(list(gcols)+[\"start_ts\"], kind=\"mergesort\")\n",
        "    episodes[\"ep_id\"] = (episodes.groupby(list(gcols), sort=False).cumcount()+1).astype(int)\n",
        "    # convert numpy datetime64 back to pandas Timestamps for downstream merges\n",
        "    for c in [\"start_ts\", \"end_last_sale_ts\", \"end_event_ts\"]:\n",
        "        if c in episodes.columns:\n",
        "            episodes[c] = pd.to_datetime(episodes[c], errors=\"coerce\")\n",
        "    return episodes\n",
        "\n",
        "\n",
        "# (Optional) Backward-compat alias if some old code still calls this name:\n",
        "# def build_episodes_fsm_from_df(*args, **kwargs):\n",
        "#     return build_episodes_with_end_event(*args, **kwargs)\n",
        "\n",
        "\n",
        "# ---------- FIXED: Annotate starts-only with dedup and safe coalescing ----------\n",
        "def annotate_starts_duration_only(df_rebased: pd.DataFrame,\n",
        "                                  episodes: pd.DataFrame,\n",
        "                                  gcols=(\"appid\",\"country\"),\n",
        "                                  time_col=\"timestamp\",\n",
        "                                  keep_dup=\"first\",\n",
        "                                  strict=False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Left with only start rows: [appid, country, timestamp, sale_ep_duration_days].\n",
        "    Coalesce both before and after the join to avoid duplicate keys.\n",
        "    \"\"\"\n",
        "    base = coalesce_by_key(df_rebased, gcols=gcols, time_col=time_col, keep=keep_dup)\n",
        "    key = (episodes[[*gcols, \"start_ts\", \"duration_days\"]]\n",
        "           .rename(columns={\"start_ts\": time_col, \"duration_days\":\"sale_ep_duration_days\"}))\n",
        "    starts_only = base.merge(key, on=[*gcols, time_col], how=\"inner\", validate=\"m:1\")\n",
        "\n",
        "    keep_cols = [\"appid\",\"country\",time_col,\"sale_ep_duration_days\"]\n",
        "    keep_cols = [c for c in keep_cols if c in starts_only.columns]\n",
        "    starts_only = (starts_only[keep_cols]\n",
        "                   .sort_values([*gcols, time_col], kind=\"mergesort\")\n",
        "                   .drop_duplicates(subset=[*gcols, time_col], keep=\"first\")\n",
        "                   .reset_index(drop=True))\n",
        "\n",
        "    if strict and starts_only.duplicated([*gcols, time_col]).any():\n",
        "        raise AssertionError(\"Duplicate (appid,country,timestamp) remain in starts_only.\")\n",
        "    return starts_only\n",
        "\n",
        "# === Make durations on df_rebased and drop explicit end events ===\n",
        "\n",
        "# --- 2) Merge inclusive duration back and DROP explicit end-event rows ---\n",
        "def make_events_with_duration(df_rebased: pd.DataFrame,\n",
        "                              tol_abs=1.0, tol_pct=0.01,\n",
        "                              gcols=(\"appid\",\"country\"), time_col=\"timestamp\",\n",
        "                              add_is_start=True, drop_ends=True,\n",
        "                              coalesce_out=True, keep_out=\"last\"):\n",
        "    \"\"\"\n",
        "    Left-merge sale_ep_duration_days (INCLUSIVE) onto df_rebased (only start rows get a value),\n",
        "    then DROP explicit end-event rows (timestamp == end_event_ts).\n",
        "    Zero-duration starts (start==end_last_sale) are kept as starts.\n",
        "    \"\"\"\n",
        "    episodes = build_episodes_with_end_event(df_rebased, gcols=gcols, time_col=time_col,\n",
        "                                             tol_abs=tol_abs, tol_pct=tol_pct)\n",
        "\n",
        "    out = df_rebased.copy()\n",
        "    out[time_col] = pd.to_datetime(out[time_col], errors=\"coerce\")\n",
        "    for c in gcols:\n",
        "        out[c] = out[c].astype(str).str.upper().str.strip()\n",
        "    if coalesce_out:\n",
        "        out = (out.sort_values(list(gcols)+[time_col], kind=\"mergesort\")\n",
        "                 .drop_duplicates(subset=[*gcols, time_col], keep=keep_out))\n",
        "\n",
        "    # write back INCLUSIVE duration to starts\n",
        "    start_key = (episodes[[*gcols, \"start_ts\", \"duration_days_inclusive\"]]\n",
        "                 .rename(columns={\"start_ts\": time_col,\n",
        "                                  \"duration_days_inclusive\":\"sale_ep_duration_days\"}))\n",
        "    out = out.merge(start_key, on=[*gcols, time_col], how=\"left\", validate=\"m:1\")\n",
        "\n",
        "    if add_is_start:\n",
        "        out[\"sale_ep_is_start\"] = out[\"sale_ep_duration_days\"].notna()\n",
        "\n",
        "    removed_ends = 0\n",
        "    if drop_ends:\n",
        "        end_key = (episodes[[*gcols, \"end_event_ts\"]].dropna()\n",
        "                   .rename(columns={\"end_event_ts\": time_col})\n",
        "                   .assign(__is_end=True))\n",
        "        out = out.merge(end_key, on=[*gcols, time_col], how=\"left\", validate=\"m:1\")\n",
        "        out[\"__is_end\"] = out[\"__is_end\"].astype(\"boolean\").fillna(False)\n",
        "        pure_end_mask = out[\"__is_end\"] & ~out[\"sale_ep_is_start\"].fillna(False)\n",
        "        removed_ends = int(pure_end_mask.sum())\n",
        "        out = out.loc[~pure_end_mask].drop(columns=[\"__is_end\"])\n",
        "\n",
        "    return out, episodes, removed_ends\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_and_save_events_with_duration(df_rebased: pd.DataFrame,\n",
        "                                        tol_abs=1.0, tol_pct=0.01,\n",
        "                                        gcols=(\"appid\",\"country\"), time_col=\"timestamp\",\n",
        "                                        out_events_csv=\"events_with_duration_no_end.csv\",\n",
        "                                        out_episodes_csv=\"episodes_duration_only.csv\",\n",
        "                                        out_starts_csv=\"starts_only_duration.csv\"):\n",
        "    \"\"\"\n",
        "    Produce three outputs:\n",
        "      - events_with_duration_no_end.csv  (df_rebased + sale_ep_duration_days; end events removed)\n",
        "      - episodes_duration_only.csv       (appid,country,start_ts,sale_ep_duration_days)\n",
        "      - starts_only_duration.csv         (appid,country,timestamp,sale_ep_duration_days)\n",
        "    Also sanity-check that starts-only rows match the number of episodes.\n",
        "    \"\"\"\n",
        "    events_with_dur, episodes, removed_ends = make_events_with_duration(\n",
        "        df_rebased, tol_abs=tol_abs, tol_pct=tol_pct,\n",
        "        gcols=gcols, time_col=time_col,\n",
        "        add_is_start=True, drop_ends=True,\n",
        "        coalesce_out=True, keep_out=\"last\"\n",
        "    )\n",
        "\n",
        "    episodes_min = (episodes[[*gcols, \"start_ts\", \"duration_days\"]]\n",
        "                    .rename(columns={\"duration_days\":\"sale_ep_duration_days\"}))\n",
        "\n",
        "    starts_only = (events_with_dur.loc[events_with_dur[\"sale_ep_duration_days\"].notna(),\n",
        "                                       [*gcols, time_col, \"sale_ep_duration_days\"]]\n",
        "                   .sort_values([*gcols, time_col], kind=\"mergesort\")\n",
        "                   .drop_duplicates(subset=[*gcols, time_col], keep=\"first\")\n",
        "                   .reset_index(drop=True))\n",
        "\n",
        "    # Sanity\n",
        "    assert len(starts_only) == len(episodes_min), \\\n",
        "        f\"Mismatch: starts_only={len(starts_only)} vs episodes={len(episodes_min)}\"\n",
        "\n",
        "    # Save CSVs (paths configurable)\n",
        "    events_with_dur.to_csv(out_events_csv, index=False)\n",
        "    episodes_min.to_csv(out_episodes_csv, index=False)\n",
        "    starts_only.to_csv(out_starts_csv, index=False)\n",
        "\n",
        "    summary = {\n",
        "        \"events_in\": int(len(df_rebased)),\n",
        "        \"episodes\": int(len(episodes_min)),\n",
        "        \"events_removed_end\": removed_ends,\n",
        "        \"events_out\": int(len(events_with_dur)),\n",
        "        \"starts_only_rows\": int(len(starts_only)),\n",
        "    }\n",
        "    return events_with_dur, episodes_min, starts_only, summary\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oG1DHobsWk5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- 3) Wrapper that saves CSVs (now using inclusive duration) ---\n",
        "def build_and_save_events_with_duration(df_rebased: pd.DataFrame,\n",
        "                                        tol_abs=1.0, tol_pct=0.01,\n",
        "                                        gcols=(\"appid\",\"country\"), time_col=\"timestamp\",\n",
        "                                        out_events_csv=\"events_with_duration_no_end.csv\",\n",
        "                                        out_episodes_csv=\"episodes_duration_only.csv\",\n",
        "                                        out_starts_csv=\"starts_only_duration.csv\"):\n",
        "    \"\"\"\n",
        "    Outputs:\n",
        "      - events_with_duration_no_end.csv  (df_rebased + sale_ep_duration_days; explicit ends removed)\n",
        "      - episodes_duration_only.csv       (appid,country,start_ts,sale_ep_duration_days)\n",
        "      - starts_only_duration.csv         (appid,country,timestamp,sale_ep_duration_days)\n",
        "    \"\"\"\n",
        "    events_with_dur, episodes, removed_ends = make_events_with_duration(\n",
        "        df_rebased, tol_abs=tol_abs, tol_pct=tol_pct,\n",
        "        gcols=gcols, time_col=time_col,\n",
        "        add_is_start=True, drop_ends=True,\n",
        "        coalesce_out=True, keep_out=\"last\"\n",
        "    )\n",
        "\n",
        "    # use inclusive duration for the minimal episode-level table\n",
        "    episodes_min = (episodes[[*gcols, \"start_ts\", \"duration_days_inclusive\"]]\n",
        "                    .rename(columns={\"duration_days_inclusive\":\"sale_ep_duration_days\"}))\n",
        "\n",
        "    starts_only = (events_with_dur.loc[events_with_dur[\"sale_ep_duration_days\"].notna(),\n",
        "                                       [*gcols, time_col, \"sale_ep_duration_days\"]]\n",
        "                   .sort_values([*gcols, time_col], kind=\"mergesort\")\n",
        "                   .drop_duplicates(subset=[*gcols, time_col], keep=\"first\")\n",
        "                   .reset_index(drop=True))\n",
        "\n",
        "    # sanity\n",
        "    assert len(starts_only) == len(episodes_min), \\\n",
        "        f\"Mismatch: starts_only={len(starts_only)} vs episodes={len(episodes_min)}\"\n",
        "\n",
        "    events_with_dur.to_csv(out_events_csv, index=False)\n",
        "    episodes_min.to_csv(out_episodes_csv, index=False)\n",
        "    starts_only.to_csv(out_starts_csv, index=False)\n",
        "\n",
        "    return (events_with_dur,\n",
        "            episodes_min,\n",
        "            starts_only,\n",
        "            {\n",
        "                \"events_in\": int(len(df_rebased)),\n",
        "                \"episodes\": int(len(episodes_min)),\n",
        "                \"events_removed_end\": removed_ends,\n",
        "                \"events_out\": int(len(events_with_dur)),\n",
        "                \"starts_only_rows\": int(len(starts_only)),\n",
        "            })\n",
        "\n",
        "\n",
        "\n",
        "def drop_neutral_rows(\n",
        "    df: pd.DataFrame,\n",
        "    cut_col: str = \"cut\",\n",
        "    flag_prefix: str = \"is_\",\n",
        "    protect_cols = (\"sale_ep_is_start\",),  # never drop episode starts\n",
        "    also_require_price_regular_match: bool = True,\n",
        "    tol_abs: float = 1.0,\n",
        "    tol_pct: float = 0.01,\n",
        ") -> tuple[pd.DataFrame, int, dict]:\n",
        "    \"\"\"\n",
        "    Remove 'neutral' rows AFTER episodes have been built and explicit ends removed.\n",
        "    A row is neutral if:\n",
        "      - cut == 0 (or NaN treated as 0), AND\n",
        "      - all boolean label flags that start with 'is_' are False, AND\n",
        "      - (optional) price ~= regular within tolerance (no price action).\n",
        "    Never drop rows marked as 'sale_ep_is_start'.\n",
        "\n",
        "    Returns: (df_clean, dropped_count, meta_info)\n",
        "    \"\"\"\n",
        "    d = df.copy()\n",
        "\n",
        "    # 1) cut == 0\n",
        "    cut_zero = pd.to_numeric(d.get(cut_col, 0), errors=\"coerce\").fillna(0).eq(0)\n",
        "\n",
        "    # 2) all 'is_*' flags are False (exclude internal markers)\n",
        "    flag_cols = [c for c in d.columns if c.startswith(flag_prefix)]\n",
        "    for ex in (\"__is_end\",):  # keep internal markers out, just in case\n",
        "        if ex in flag_cols:\n",
        "            flag_cols.remove(ex)\n",
        "\n",
        "    if flag_cols:\n",
        "        flags_any_true = np.column_stack(\n",
        "            [pd.to_numeric(d[c], errors=\"coerce\").fillna(0).astype(bool) for c in flag_cols]\n",
        "        ).any(axis=1)\n",
        "    else:\n",
        "        flags_any_true = np.zeros(len(d), dtype=bool)\n",
        "\n",
        "    # 3) optional: no price move relative to regular under tolerance\n",
        "    if also_require_price_regular_match and {\"price\", \"regular\"} <= set(d.columns):\n",
        "        p = pd.to_numeric(d[\"price\"], errors=\"coerce\")\n",
        "        r = pd.to_numeric(d[\"regular\"], errors=\"coerce\")\n",
        "        thr = np.maximum(tol_abs, tol_pct * r)\n",
        "        no_price_move = (p.notna() & r.notna() & (r > 0) & ((r - p).abs() <= thr)) | r.isna() | p.isna()\n",
        "    else:\n",
        "        no_price_move = np.ones(len(d), dtype=bool)\n",
        "\n",
        "    # 4) never drop episode starts\n",
        "    protect_start = d.get(\"sale_ep_is_start\", False)\n",
        "    if isinstance(protect_start, pd.Series):\n",
        "        protect_start = protect_start.fillna(False).astype(bool)\n",
        "    else:\n",
        "        protect_start = np.zeros(len(d), dtype=bool)\n",
        "\n",
        "    # 5) build mask and apply\n",
        "    drop_mask = cut_zero & (~flags_any_true) & no_price_move & (~protect_start)\n",
        "    dropped = int(drop_mask.sum())\n",
        "    d_clean = d.loc[~drop_mask].reset_index(drop=True)\n",
        "\n",
        "    meta = {\n",
        "        \"checked_flag_cols\": flag_cols,\n",
        "        \"tol_abs\": tol_abs,\n",
        "        \"tol_pct\": tol_pct,\n",
        "    }\n",
        "    return d_clean, dropped, meta\n",
        "\n"
      ],
      "metadata": {
        "id": "6oa5SxKNpFmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Caller script: build episodes, write duration, drop explicit ends, then drop neutral rows ===\n",
        "# All comments in English as requested.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 0) Safety: require df_rebased in memory\n",
        "if \"df_rebased\" not in globals():\n",
        "    raise RuntimeError(\"`df_rebased` not found. Run your Module-4 pipeline first to create it in memory.\")\n",
        "\n",
        "# 1) Build episodes (inclusive), write duration to starts, drop explicit end-event rows, save CSVs\n",
        "events_with_dur, episodes_min, starts_only, summary = build_and_save_events_with_duration(\n",
        "    df_rebased,\n",
        "    tol_abs=1.0,           # absolute tolerance (release currency units)\n",
        "    tol_pct=0.01,          # relative tolerance (1%)\n",
        "    gcols=(\"appid\",\"country\"),\n",
        "    time_col=\"timestamp\",\n",
        "    out_events_csv=\"events_with_duration_no_end.csv\",\n",
        "    out_episodes_csv=\"episodes_duration_only.csv\",\n",
        "    out_starts_csv=\"starts_only_duration.csv\",\n",
        ")\n",
        "\n",
        "# 2) Quick overview (same spirit as your screenshot)\n",
        "print(\"Quick Overview (pre-clean)\")\n",
        "print({k:int(v) for k,v in summary.items()})\n",
        "\n",
        "by_country = (\n",
        "    episodes_min.groupby(\"country\", dropna=False)\n",
        "    .size().reset_index(name=\"episodes\")\n",
        "    .sort_values(\"episodes\", ascending=False, kind=\"mergesort\")\n",
        ")\n",
        "print(\"\\nEpisode count by country (top 20):\")\n",
        "print(by_country.head(20).to_string(index=False))\n",
        "\n",
        "print(\"\\nEpisodes (head 20):\")\n",
        "print(episodes_min.head(20).to_string(index=False))\n",
        "\n",
        "# 3) Consistency checks (asserts) on the pre-clean dataset\n",
        "assert len(starts_only) == len(episodes_min), \"starts_only rows must equal episodes\"\n",
        "\n",
        "def _audit_no_explicit_ends(ev: pd.DataFrame, base: pd.DataFrame) -> int:\n",
        "    \"\"\"Verify there are no pure explicit end-event rows left in `ev`.\"\"\"\n",
        "    eps_full = build_episodes_with_end_event(base, gcols=(\"appid\",\"country\"), time_col=\"timestamp\",\n",
        "                                             tol_abs=1.0, tol_pct=0.01)\n",
        "    end_key = (eps_full[[\"appid\",\"country\",\"end_event_ts\"]].dropna()\n",
        "               .rename(columns={\"end_event_ts\":\"timestamp\"})\n",
        "               .assign(__end_marker=True))\n",
        "    chk = ev.merge(end_key, on=[\"appid\",\"country\",\"timestamp\"], how=\"left\")\n",
        "    # use nullable boolean to avoid FutureWarning\n",
        "    pure_end = chk[\"__end_marker\"].astype(\"boolean\").fillna(False) & ~chk.get(\"sale_ep_is_start\", False).fillna(False)\n",
        "    return int(pure_end.sum())\n",
        "\n",
        "pure_end_left = _audit_no_explicit_ends(events_with_dur, df_rebased)\n",
        "print(\"\\nPure explicit end-event rows remaining in events_with_dur (pre-clean):\", pure_end_left)\n",
        "assert pure_end_left == 0, \"There are still explicit end-event rows present.\"\n",
        "\n",
        "# 4) Neutral cleanup (must be AFTER explicit ends are removed)\n",
        "events_clean, neutral_dropped, meta = drop_neutral_rows(\n",
        "    events_with_dur,\n",
        "    cut_col=\"cut\",\n",
        "    flag_prefix=\"is_\",\n",
        "    protect_cols=(\"sale_ep_is_start\",),     # protects episode starts\n",
        "    also_require_price_regular_match=True,  # safer: also require price~regular within tolerance\n",
        "    tol_abs=1.0,\n",
        "    tol_pct=0.01,\n",
        ")\n",
        "events_clean.to_csv(\"events_with_duration_no_end_clean.csv\", index=False)\n",
        "\n",
        "# 5) Post-clean sanity and summary\n",
        "# 5.1 No explicit end rows (should remain zero)\n",
        "pure_end_left_after = _audit_no_explicit_ends(events_clean, df_rebased)\n",
        "assert pure_end_left_after == 0, \"End rows leaked after neutral cleanup.\"\n",
        "\n",
        "# 5.2 starts count unchanged\n",
        "assert events_clean[\"sale_ep_is_start\"].fillna(False).sum() == len(episodes_min), \\\n",
        "    \"Number of start rows changed after neutral cleanup.\"\n",
        "\n",
        "# 5.3 Update and print final summary\n",
        "summary[\"neutral_removed\"] = int(neutral_dropped)\n",
        "summary[\"events_out_clean\"] = int(len(events_clean))\n",
        "\n",
        "print(\"\\nNeutral rows removed:\", neutral_dropped)\n",
        "print(\"Saved: events_with_duration_no_end_clean.csv\")\n",
        "\n",
        "print(\"\\nQuick Overview (post-clean)\")\n",
        "print({k:int(v) for k,v in summary.items()})\n",
        "\n",
        "# 6) (Optional) a tiny head to eyeball\n",
        "print(\"\\nClean sample (head 10):\")\n",
        "print(events_clean.head(10)[[\"appid\",\"country\",\"timestamp\",\"price\",\"regular\",\"cut\",\n",
        "                             \"sale_ep_is_start\",\"sale_ep_duration_days\"]].to_string(index=False))\n",
        "\n",
        "# Files produced in working directory:\n",
        "#  - events_with_duration_no_end.csv           (episode duration annotated; explicit ends removed)\n",
        "#  - episodes_duration_only.csv                (appid,country,start_ts,sale_ep_duration_days)\n",
        "#  - starts_only_duration.csv                  (appid,country,timestamp,sale_ep_duration_days)\n",
        "#  - events_with_duration_no_end_clean.csv     (further removed neutral \"no-op\" rows)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYt5YBl4gcA1",
        "outputId": "6dae2215-f455-437f-e075-27e2329e8344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quick Overview (pre-clean)\n",
            "{'events_in': 17895, 'episodes': 8476, 'events_removed_end': 8452, 'events_out': 9442, 'starts_only_rows': 8476}\n",
            "\n",
            "Episode count by country (top 20):\n",
            "country  episodes\n",
            "     BR       761\n",
            "     AR       760\n",
            "     US       760\n",
            "     AU       759\n",
            "     GB       756\n",
            "     CA       755\n",
            "     TR       753\n",
            "     DE       747\n",
            "     FR       747\n",
            "     PL       747\n",
            "     CN       717\n",
            "     JP       214\n",
            "\n",
            "Episodes (head 20):\n",
            "  appid country            start_ts  sale_ep_duration_days\n",
            "1086940      AR 2023-12-21 20:22:31              13.977488\n",
            "1086940      AR 2024-03-14 17:37:53               7.005648\n",
            "1086940      AR 2024-05-16 17:18:11               6.998171\n",
            "1086940      AR 2024-06-27 17:59:38              16.981331\n",
            "1086940      AR 2024-09-05 17:18:42               4.002419\n",
            "1086940      AR 2024-11-04 18:18:01               7.002465\n",
            "1086940      AR 2024-11-27 21:03:11               6.901956\n",
            "1086940      AR 2024-12-19 21:27:38              13.888889\n",
            "1086940      AR 2025-02-10 18:32:12               6.998611\n",
            "1086940      AR 2025-03-13 19:23:51               6.928854\n",
            "1086940      AR 2025-04-07 17:20:08              13.998738\n",
            "1086940      AR 2025-05-19 17:42:20               7.072940\n",
            "1086940      AR 2025-06-26 21:43:10              13.832373\n",
            "1086940      AR 2025-09-01 17:22:15               7.000185\n",
            "1086940      AR 2025-09-29 21:03:02               6.860069\n",
            "1086940      AU 2023-12-21 20:23:12              13.982708\n",
            "1086940      AU 2024-03-14 17:43:33               7.001528\n",
            "1086940      AU 2024-05-16 17:18:23               6.999954\n",
            "1086940      AU 2024-06-27 17:59:37              16.983183\n",
            "1086940      AU 2024-09-05 17:18:39               4.002315\n",
            "\n",
            "Pure explicit end-event rows remaining in events_with_dur (pre-clean): 0\n",
            "\n",
            "Neutral rows removed: 57\n",
            "Saved: events_with_duration_no_end_clean.csv\n",
            "\n",
            "Quick Overview (post-clean)\n",
            "{'events_in': 17895, 'episodes': 8476, 'events_removed_end': 8452, 'events_out': 9442, 'starts_only_rows': 8476, 'neutral_removed': 57, 'events_out_clean': 9385}\n",
            "\n",
            "Clean sample (head 10):\n",
            "  appid country           timestamp       price     regular  cut  sale_ep_is_start  sale_ep_duration_days\n",
            "1030300      AR 2025-09-04 14:33:21   19.990000   19.990000  0.0             False                    NaN\n",
            "1030300      AU 2025-09-04 14:33:31   29.500000   29.500000  0.0             False                    NaN\n",
            "1030300      BR 2025-09-04 14:33:10   59.990000   59.990000  0.0             False                    NaN\n",
            "1030300      CA 2025-09-04 14:30:33   25.990000   25.990000  0.0             False                    NaN\n",
            "1030300      CN 2025-09-04 14:33:29   76.000000   76.000000  0.0             False                    NaN\n",
            "1030300      DE 2025-09-04 14:30:21   19.500000   19.500000  0.0             False                    NaN\n",
            "1030300      FR 2025-09-04 14:30:21   19.500000   19.500000  0.0             False                    NaN\n",
            "1030300      GB 2025-09-04 14:33:27   16.750000   16.750000  0.0             False                    NaN\n",
            "1030300      JP 2025-09-04 14:30:34 2300.000000 2300.000000  0.0             False                    NaN\n",
            "1030300      PL 2025-09-04 14:30:21   81.823529   81.823529  0.0             False                    NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- 1) Build a full episode table once (if not already available) ---\n",
        "def build_episodes_with_end_event(df: pd.DataFrame,\n",
        "                                  gcols=(\"appid\",\"country\"),\n",
        "                                  time_col=\"timestamp\",\n",
        "                                  tol_abs: float = 1.0,\n",
        "                                  tol_pct: float = 0.01) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Return columns: appid, country, start_ts, end_last_sale_ts, end_event_ts, duration_days_inclusive\n",
        "    Sale-state uses tolerant same-currency logic; inclusive duration uses the explicit end row when present.\n",
        "    \"\"\"\n",
        "    d = df.copy()\n",
        "    d[time_col] = pd.to_datetime(d[time_col], errors=\"coerce\")\n",
        "    for c in gcols + (\"currency\",\"release_currency\"):\n",
        "        if c in d.columns:\n",
        "            d[c] = d[c].astype(str).str.upper().str.strip()\n",
        "    d = d.sort_values(list(gcols)+[time_col], kind=\"mergesort\")\n",
        "\n",
        "    # sale_state: both thresholds; cut>0 votes\n",
        "    p = pd.to_numeric(d[\"price\"], errors=\"coerce\")\n",
        "    r = pd.to_numeric(d[\"regular\"], errors=\"coerce\")\n",
        "    thr = np.maximum(tol_abs, tol_pct * r)\n",
        "    sale = (p.notna() & r.notna() & r.gt(0) & ((r - p) > thr)) | (pd.to_numeric(d.get(\"cut\", 0), errors=\"coerce\").fillna(0) > 0)\n",
        "    d[\"sale_state\"] = sale.fillna(False)\n",
        "\n",
        "    out = []\n",
        "    for (a,c), g in d.groupby(list(gcols), sort=False):\n",
        "        ts = pd.to_datetime(g[time_col]).to_numpy(\"datetime64[ns]\")\n",
        "        ss = g[\"sale_state\"].to_numpy(bool)\n",
        "        in_sale = False; start_idx = None\n",
        "        for i in range(len(g)):\n",
        "            if (not in_sale) and ss[i]:\n",
        "                in_sale = True; start_idx = i\n",
        "            elif in_sale and (not ss[i]):\n",
        "                end_last_sale_idx = i - 1\n",
        "                dur_inc_days = (ts[i] - ts[start_idx]).astype(\"timedelta64[s]\").astype(float)/86400.0\n",
        "                out.append({\n",
        "                    \"appid\": str(a), \"country\": str(c),\n",
        "                    \"start_ts\": ts[start_idx].astype(\"datetime64[ns]\"),\n",
        "                    \"end_last_sale_ts\": ts[end_last_sale_idx].astype(\"datetime64[ns]\"),\n",
        "                    \"end_event_ts\": ts[i].astype(\"datetime64[ns]\"),\n",
        "                    \"duration_days_inclusive\": dur_inc_days,\n",
        "                })\n",
        "                in_sale = False; start_idx = None\n",
        "        if in_sale and start_idx is not None:\n",
        "            end_last_sale_idx = len(g) - 1\n",
        "            dur_open_days = (ts[end_last_sale_idx] - ts[start_idx]).astype(\"timedelta64[s]\").astype(float)/86400.0\n",
        "            out.append({\n",
        "                \"appid\": str(a), \"country\": str(c),\n",
        "                \"start_ts\": ts[start_idx].astype(\"datetime64[ns]\"),\n",
        "                \"end_last_sale_ts\": ts[end_last_sale_idx].astype(\"datetime64[ns]\"),\n",
        "                \"end_event_ts\": pd.NaT,\n",
        "                \"duration_days_inclusive\": dur_open_days,\n",
        "            })\n",
        "    eps = pd.DataFrame(out)\n",
        "    if not eps.empty:\n",
        "        eps = eps.sort_values([\"appid\",\"country\",\"start_ts\"], kind=\"mergesort\").reset_index(drop=True)\n",
        "    return eps\n",
        "\n",
        "# --- 2) Propagate duration to all sale rows (group-wise; no merge_asof) ---\n",
        "def propagate_duration_to_all_sale_rows_clean(\n",
        "    events_clean: pd.DataFrame,\n",
        "    episodes_full: pd.DataFrame,\n",
        "    gcols=(\"appid\",\"country\"),\n",
        "    time_col=\"timestamp\",\n",
        "    only_fill_cut_pos: bool = True,\n",
        ") -> tuple[pd.DataFrame, int]:\n",
        "    \"\"\"\n",
        "    For each (appid,country), map each event row to the latest episode whose start_ts <= timestamp,\n",
        "    then keep rows with timestamp <= end_last_sale_ts. Fill sale_ep_duration_days for non-start rows.\n",
        "    Stable and fast; no merge_asof; avoids sorted-key and column-order pitfalls.\n",
        "    \"\"\"\n",
        "    if episodes_full is None or episodes_full.empty:\n",
        "        return events_clean.copy(), 0\n",
        "\n",
        "    ev = events_clean.copy()\n",
        "    ev[time_col] = pd.to_datetime(ev[time_col], errors=\"coerce\")\n",
        "    for col in gcols:\n",
        "        ev[col] = ev[col].astype(str).str.upper().str.strip()\n",
        "\n",
        "    ep = episodes_full.copy()\n",
        "    ep[\"start_ts\"] = pd.to_datetime(ep[\"start_ts\"], errors=\"coerce\")\n",
        "    ep[\"end_last_sale_ts\"] = pd.to_datetime(ep[\"end_last_sale_ts\"], errors=\"coerce\")\n",
        "    for col in gcols:\n",
        "        ep[col] = ep[col].astype(str).str.upper().str.strip()\n",
        "\n",
        "    filled_total = 0\n",
        "    chunks = []\n",
        "\n",
        "    for (a,c), ev_g in ev.groupby(list(gcols), sort=False):\n",
        "        ep_g = ep[(ep[\"appid\"]==str(a)) & (ep[\"country\"]==str(c))]\n",
        "        if ep_g.empty:\n",
        "            chunks.append(ev_g); continue\n",
        "\n",
        "        # sort by time\n",
        "        ev_g = ev_g.sort_values(time_col, kind=\"mergesort\").reset_index(drop=True)\n",
        "        ep_g = ep_g.sort_values(\"start_ts\", kind=\"mergesort\").reset_index(drop=True)\n",
        "\n",
        "        t = ev_g[time_col].to_numpy(\"datetime64[ns]\")\n",
        "        s = ep_g[\"start_ts\"].to_numpy(\"datetime64[ns]\")\n",
        "        e = ep_g[\"end_last_sale_ts\"].to_numpy(\"datetime64[ns]\")\n",
        "        d = ep_g[\"duration_days_inclusive\"].to_numpy(float)\n",
        "\n",
        "        # for each event time, find index of last episode with start_ts <= t\n",
        "        pos = np.searchsorted(s, t, side=\"right\") - 1\n",
        "        valid = (pos >= 0)\n",
        "        # membership: t <= end_last_sale_ts of that episode\n",
        "        member = np.zeros(len(ev_g), dtype=bool)\n",
        "        member[valid] = t[valid] <= e[pos[valid]]\n",
        "\n",
        "        # fill mask: non-start & member & (optional) cut>0 & duration is NaN\n",
        "        non_start = ~ev_g.get(\"sale_ep_is_start\", False).fillna(False).to_numpy()\n",
        "        if only_fill_cut_pos and (\"cut\" in ev_g.columns):\n",
        "            cut_pos = pd.to_numeric(ev_g[\"cut\"], errors=\"coerce\").fillna(0).to_numpy() > 0\n",
        "        else:\n",
        "            cut_pos = np.ones(len(ev_g), dtype=bool)\n",
        "        need_fill = non_start & member & cut_pos & ev_g[\"sale_ep_duration_days\"].isna().to_numpy()\n",
        "\n",
        "        # assign duration\n",
        "        dur_vals = np.full(len(ev_g), np.nan)\n",
        "        ok_idx = np.where(need_fill)[0]\n",
        "        if len(ok_idx):\n",
        "            dur_vals[ok_idx] = d[pos[ok_idx]]\n",
        "            ev_g.loc[need_fill, \"sale_ep_duration_days\"] = dur_vals[need_fill]\n",
        "            filled_total += int(need_fill.sum())\n",
        "\n",
        "        chunks.append(ev_g)\n",
        "\n",
        "    out = pd.concat(chunks, axis=0, ignore_index=True)\n",
        "    return out, filled_total\n",
        "\n",
        "# --- 3) Simple audits that reuse episodes_full (no rebuild) ---\n",
        "def audit_no_explicit_ends(evs: pd.DataFrame, eps_full: pd.DataFrame,\n",
        "                           gcols=(\"appid\",\"country\"), time_col=\"timestamp\") -> int:\n",
        "    key = (eps_full[[*gcols,\"end_event_ts\"]].dropna()\n",
        "           .rename(columns={\"end_event_ts\": time_col})\n",
        "           .assign(__end=True))\n",
        "    merged = evs.merge(key, on=[*gcols, time_col], how=\"left\", validate=\"m:1\")\n",
        "    pure_end = merged[\"__end\"].astype(\"boolean\").fillna(False) & ~merged.get(\"sale_ep_is_start\", False).fillna(False)\n",
        "    return int(pure_end.sum())\n",
        "# Build full episodes once (if not already built in the session)\n",
        "episodes_full = build_episodes_with_end_event(df_rebased, gcols=(\"appid\",\"country\"), time_col=\"timestamp\",\n",
        "                                              tol_abs=1.0, tol_pct=0.01)\n",
        "\n",
        "# Propagate on CLEAN\n",
        "events_clean = pd.read_csv(\"events_with_duration_no_end_clean.csv\", parse_dates=[\"timestamp\"])\n",
        "events_clean_prop, filled_cnt = propagate_duration_to_all_sale_rows_clean(\n",
        "    events_clean, episodes_full, gcols=(\"appid\",\"country\"), time_col=\"timestamp\", only_fill_cut_pos=True\n",
        ")\n",
        "print(\"Filled non-start sale rows on CLEAN:\", filled_cnt)\n",
        "# safety: starts count unchanged; no explicit end reintroduced\n",
        "assert events_clean_prop[\"sale_ep_is_start\"].fillna(False).sum() == len(pd.read_csv(\"episodes_duration_only.csv\"))\n",
        "assert audit_no_explicit_ends(events_clean_prop, episodes_full) == 0\n",
        "\n",
        "events_clean_prop.to_csv(\"events_with_duration_no_end_clean_propagated.csv\", index=False)\n",
        "print(\"Saved: events_with_duration_no_end_clean_propagated.csv\")"
      ],
      "metadata": {
        "id": "VxjGOdBevTTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(episodes_min[\"sale_ep_duration_days\"] == 0).mean()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQmrkIqAdUTa",
        "outputId": "976c147a-cf5d-469c-de0a-55c29d3ac10f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.002831524303916942)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "episodes_min.groupby(\"country\")[\"sale_ep_duration_days\"].quantile([.5,.9,.99]).unstack()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "VBfMOiPNdekB",
        "outputId": "35000ebc-ca88-4264-c162-cb601dd10ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0.50       0.90       0.99\n",
              "country                                \n",
              "AR       7.044606  14.004135  23.417762\n",
              "AU       7.072986  14.002655  25.593628\n",
              "BR       7.090625  14.005856  27.978560\n",
              "CA       7.080856  14.005664  27.786109\n",
              "CN       7.997697  14.004377  27.175597\n",
              "...           ...        ...        ...\n",
              "GB       7.093119  14.008762  27.986126\n",
              "JP       9.988681  14.002337  25.826134\n",
              "PL       7.050741  14.004218  27.787101\n",
              "TR       7.072141  14.002970  27.777685\n",
              "US       7.044606  14.004135  23.417762\n",
              "\n",
              "[12 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-81d0644e-97c9-4631-adc9-56f76b1deb9a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0.50</th>\n",
              "      <th>0.90</th>\n",
              "      <th>0.99</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>AR</th>\n",
              "      <td>7.044606</td>\n",
              "      <td>14.004135</td>\n",
              "      <td>23.417762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AU</th>\n",
              "      <td>7.072986</td>\n",
              "      <td>14.002655</td>\n",
              "      <td>25.593628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BR</th>\n",
              "      <td>7.090625</td>\n",
              "      <td>14.005856</td>\n",
              "      <td>27.978560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CA</th>\n",
              "      <td>7.080856</td>\n",
              "      <td>14.005664</td>\n",
              "      <td>27.786109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CN</th>\n",
              "      <td>7.997697</td>\n",
              "      <td>14.004377</td>\n",
              "      <td>27.175597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GB</th>\n",
              "      <td>7.093119</td>\n",
              "      <td>14.008762</td>\n",
              "      <td>27.986126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>JP</th>\n",
              "      <td>9.988681</td>\n",
              "      <td>14.002337</td>\n",
              "      <td>25.826134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PL</th>\n",
              "      <td>7.050741</td>\n",
              "      <td>14.004218</td>\n",
              "      <td>27.787101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TR</th>\n",
              "      <td>7.072141</td>\n",
              "      <td>14.002970</td>\n",
              "      <td>27.777685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>US</th>\n",
              "      <td>7.044606</td>\n",
              "      <td>14.004135</td>\n",
              "      <td>23.417762</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-81d0644e-97c9-4631-adc9-56f76b1deb9a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-81d0644e-97c9-4631-adc9-56f76b1deb9a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-81d0644e-97c9-4631-adc9-56f76b1deb9a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-16ea2273-ebca-4ff8-891e-3ba1ee1fc9dc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-16ea2273-ebca-4ff8-891e-3ba1ee1fc9dc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-16ea2273-ebca-4ff8-891e-3ba1ee1fc9dc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"episodes_min\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"country\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"TR\",\n          \"PL\",\n          \"AR\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.5,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.86239808224013,\n        \"min\": 7.044606481481482,\n        \"max\": 9.988680555555556,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          9.988680555555556,\n          7.072986111111111,\n          7.050740740740741\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.9,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0017131566204718676,\n        \"min\": 14.002336805555554,\n        \"max\": 14.008761574074075,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          14.002336805555554,\n          14.002655092592592,\n          14.004217592592592\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.99,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.7303384332672993,\n        \"min\": 23.417762268518487,\n        \"max\": 27.986125578703707,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          25.826133564814825,\n          25.593627546296144,\n          27.78710138888889\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    }
  ]
}